{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc486a68-32b4-49ee-b83f-c737ee0081e9",
   "metadata": {
    "id": "bc486a68-32b4-49ee-b83f-c737ee0081e9"
   },
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 5: Natural Language Generation and Conversion with Transformer\n",
    "\n",
    "## 5.3 Kana-Kanji Conversion with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d57d9-8719-4711-b4b9-32e606a8c0bd",
   "metadata": {
    "id": "427d57d9-8719-4711-b4b9-32e606a8c0bd"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"link.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf997b-f957-4977-ae2e-91e51a46ad8b",
   "metadata": {
    "id": "cfdf997b-f957-4977-ae2e-91e51a46ad8b"
   },
   "source": [
    "* [Imports and Setup](#setup)\n",
    "* [5.3 Kana-Kanji Conversion with Transformer](#5.3)\n",
    "    - [5.3.1 Sequence to Sequence (Seq2Seq) Models](#5.3.1)\n",
    "    - [5.3.2 Converting from Kanji-Kana majiribun into Romaji](#5.3.2)\n",
    "    - [5.3.3 Training and Tokenizing with SentencePiece](#5.3.3)\n",
    "    - [5.3.4 Training a Conversion Model with Fairseq](#5.3.4)\n",
    "    - [5.3.5 Checking created artifacts](#5.3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22c158-28dd-47cf-a0f8-01c530591cf3",
   "metadata": {
    "id": "4f22c158-28dd-47cf-a0f8-01c530591cf3"
   },
   "source": [
    "---\n",
    "<a name='setup'></a><a id='setup'></a>\n",
    "# Imports and Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pnRcn305Ngsj",
   "metadata": {
    "id": "pnRcn305Ngsj"
   },
   "outputs": [],
   "source": [
    "# Option to use downloaded/pre-trained data (assumes Colab platform)\n",
    "USE_GD_DATA = False\n",
    "\n",
    "if USE_GD_DATA:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        print(\"Creating a local copy of chp05_03...\")\n",
    "        # Assumes prepared data is stored as 'My Drive/chp05_03' on Google Drive \n",
    "        !cp -r /content/drive/MyDrive/chp05_03 /content/chp05_03\n",
    "        print()\n",
    "        !ls -l /content/chp05_03\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d218a66-cb88-48b7-b446-f78f2e9ac892",
   "metadata": {
    "id": "8d218a66-cb88-48b7-b446-f78f2e9ac892"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(\"chp05_03\")\n",
    "req_file = data_root / \"requirements_5_5_3.txt\"\n",
    "\n",
    "if not data_root.is_dir():\n",
    "    data_root.mkdir()\n",
    "else:\n",
    "    print(f\"{data_root} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Cd6jWZ1vmBrd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cd6jWZ1vmBrd",
    "outputId": "f25042b5-bd87-4fea-ed9e-55ae959cd9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chp05_03/requirements_5_5_3.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "cutlet==0.1.19\n",
    "fugashi[unidic]==1.2.1\n",
    "sentencepiece==0.1.97\n",
    "fairseq==0.12.2\n",
    "tensorboardX==2.5.1\n",
    "watermark==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "EO3VuT9c857f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EO3VuT9c857f",
    "outputId": "daa5dbfe-7427-4e85-d5f4-3c9027a8788f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages\n",
      "\u001b[K     |████████████████████████████████| 364 kB 19.8 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 615 kB 56.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 63.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 58.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 125 kB 81.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 128 kB 72.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 241 kB 81.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 123 kB 82.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 118 kB 79.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 112 kB 68.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 63.2 MB/s \n",
      "\u001b[?25h  Building wheel for cutlet (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic-3.1.0.zip\n",
      "Dictionary version: 3.1.0+2021-08-31\n",
      "Downloading UniDic v3.1.0+2021-08-31...\n",
      "unidic-3.1.0.zip: 100% 526M/526M [00:13<00:00, 37.9MB/s]\n",
      "Finished download.\n",
      "Downloaded UniDic v3.1.0+2021-08-31 to /usr/local/lib/python3.8/dist-packages/unidic/dicdir\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package libfile-next-perl.\n",
      "(Reading database ... 124022 files and directories currently installed.)\n",
      "Preparing to unpack .../libfile-next-perl_1.16-2_all.deb ...\n",
      "Unpacking libfile-next-perl (1.16-2) ...\n",
      "Selecting previously unselected package ack.\n",
      "Preparing to unpack .../archives/ack_2.22-1_all.deb ...\n",
      "Unpacking ack (2.22-1) ...\n",
      "Setting up libfile-next-perl (1.16-2) ...\n",
      "Setting up ack (2.22-1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "\n",
      "** Need to restart runtime after installing sentencepiece **\n",
      "> Runtime > Restart runtime ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "check1 = ('google.colab' in sys.modules)\n",
    "check2 = (os.environ.get('CLOUDSDK_CONFIG')=='/content/.config')\n",
    "IS_COLAB = True if (check1 or check2) else False\n",
    "\n",
    "# Need fugashi for cutlet, and need unidic for fugashi\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --quiet -r {req_file}\n",
    "    !apt-get install tree &> /dev/null\n",
    "    !python -m unidic download\n",
    "    !sudo apt-get install ack -qq\n",
    "    print()\n",
    "    print(\"** Need to restart runtime after installing sentencepiece **\")\n",
    "    print(\"> Runtime > Restart runtime ...\")\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tTaKzm4amWGp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTaKzm4amWGp",
    "outputId": "cc223c7a-6683-4637-dd0d-a3daff81fb2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_COLAB: True\n",
      "device:cuda\n",
      "--------------------------------------------------\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.15\n",
      "IPython version      : 7.9.0\n",
      "\n",
      "cutlet       : 0.1.19\n",
      "fairseq      : 0.12.2\n",
      "fugashi      : 1.2.1\n",
      "sentencepiece: 0.1.97\n",
      "tensorboardX : 2.5.1\n",
      "torch        : 1.12.1+cu113\n",
      "watermark    : 2.3.1\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.10.133+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard Library imports\n",
    "from itertools import chain\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shlex\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    " \n",
    "# Third-party imports\n",
    "import cutlet\n",
    "import fairseq\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from watermark import watermark\n",
    "\n",
    "# Suppress TensorFlog log messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "check1 = ('google.colab' in sys.modules)\n",
    "check2 = (os.environ.get('CLOUDSDK_CONFIG')=='/content/.config')\n",
    "IS_COLAB = True if (check1 or check2) else False\n",
    "print(f\"IS_COLAB: {IS_COLAB}\")\n",
    "\n",
    "katsu = cutlet.Cutlet()\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*50)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device:{device}\")\n",
    "HR()\n",
    "\n",
    "packages_check=\"cutlet,fairseq,fugashi,sentencepiece,tensorboardX,torch,watermark\"\n",
    "print(watermark(packages=packages_check, python=True,machine=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a526fe-b636-4f69-b294-742f65537ffb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5a526fe-b636-4f69-b294-742f65537ffb",
    "outputId": "1fc9c58b-15d7-413d-9983-cbbe824f2529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data_root:\tchp05_03\n",
      "data_file:\tsentences20210924.tar.bz2\n",
      "data_url:\tjanlpbook.s3.amazonaws.com/sentences20210924.tar.bz2\n",
      "data_dir:\tchp05_03/data\n",
      "data_src:\tchp05_03/data/sentences20210924.tar.bz2\n",
      "data_path:\tchp05_03/data/sentences.csv\n",
      "\n",
      "data_file2:\tcc100.ja.mod1k.txt\n",
      "data_url2:\tjanlpbook.s3.amazonaws.com/cc100.ja.mod1k.txt\n",
      "data_path2:\tchp05_03/data/cc100.ja.mod1k.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = \"sentences20210924.tar.bz2\"\n",
    "data_url = f\"janlpbook.s3.amazonaws.com/{data_file}\"\n",
    "data_root = Path(\"chp05_03\") # redefine after runtime restart\n",
    "data_dir = data_root /\"data\"\n",
    "data_src = data_dir / data_file\n",
    "data_path = data_dir / \"sentences.csv\"\n",
    "\n",
    "data_file2 = \"cc100.ja.mod1k.txt\"\n",
    "data_url2 = f\"janlpbook.s3.amazonaws.com/{data_file2}\"\n",
    "data_path2 = data_dir / data_file2\n",
    "\n",
    "print(f\"\"\"\n",
    "data_root:\\t{data_root}\n",
    "data_file:\\t{data_file}\n",
    "data_url:\\t{data_url}\n",
    "data_dir:\\t{data_dir}\n",
    "data_src:\\t{data_src}\n",
    "data_path:\\t{data_path}\n",
    "\n",
    "data_file2:\\t{data_file2}\n",
    "data_url2:\\t{data_url2}\n",
    "data_path2:\\t{data_path2}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87a8f-a8b1-4782-b2aa-e07d52555d3c",
   "metadata": {
    "id": "a3e87a8f-a8b1-4782-b2aa-e07d52555d3c"
   },
   "source": [
    "---\n",
    "<a name='5.3'></a><a id='5.3'></a>\n",
    "# 5.3 Kana-Kanji Conversion with Transformer\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c546610-4647-48cb-98d1-1704f73ec8cd",
   "metadata": {
    "id": "3c546610-4647-48cb-98d1-1704f73ec8cd"
   },
   "source": [
    "<a name='5.3.1'></a><a id='5.3.1'></a>\n",
    "## 5.3.1 Sequence to Sequence (Seq2Seq) Models\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjCOtrByjQ3G",
   "metadata": {
    "id": "GjCOtrByjQ3G"
   },
   "source": [
    "* For the Kana-Kanji conversion, use a Transformer-based Seq2Seq model.\n",
    "* A Seq2Seq model converts a sequence into another sequence.\n",
    "* It consists of two subcomponents, each of which is usually a full neural network with multiple layers:\n",
    "    - An encoder\n",
    "    - A decoder\n",
    "* The encoder converts input into an internal representation, similar to word embeddings. \n",
    "* The decoder takes the representations produced by the encoder and produces the output text.\n",
    "* Here we use a Seq2Seq model with Transformer-based architecture.\n",
    "* Fairseq implements Transformer Seq2Seq, and we will use one of its default Transformer configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a8660-2df5-479e-9321-1b7b0b991ac5",
   "metadata": {
    "id": "643a8660-2df5-479e-9321-1b7b0b991ac5"
   },
   "source": [
    "<a name='5.3.2'></a><a id='5.3.2'></a>\n",
    "## 5.3.2 Converting from Kanji-Kana majiribun into Romaji\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* Create the parallel corpus for Kana-Kanji conversion\n",
    "* Use cutlet to convert Kanji-Kana majiribun to Romaji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d681294-818b-4779-a6a3-ff9d4c5dad97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "3d681294-818b-4779-a6a3-ff9d4c5dad97",
    "outputId": "401edd39-40d0-4ffa-8655-fa33de109c49"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Katsu karee wa oishii'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hello-world example\n",
    "katsu = cutlet.Cutlet()\n",
    "katsu.use_foreign_spelling = False\n",
    "katsu.romaji(\"カツカレーは美味しい\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007e7a5-4277-4ef0-adcd-d87432574e24",
   "metadata": {
    "id": "8007e7a5-4277-4ef0-adcd-d87432574e24"
   },
   "source": [
    "* Create a large corpus of raw Japanese texts in order to build a parallel (Romaji to Japanese) corpus.\n",
    "* Use a combination of Tatoeba and CC-100 datasets.\n",
    "* Use a 1/1000 sample of CC-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4beb1619-71e9-4871-84bb-12ffe21342ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4beb1619-71e9-4871-84bb-12ffe21342ca",
    "outputId": "502f2eaa-ea54-4d30-cfd3-c340d3f23d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: chp05_03/data\n"
     ]
    }
   ],
   "source": [
    "if not (data_dir).is_dir():\n",
    "    print(f\"Creating: {data_dir}\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=False)\n",
    "else:\n",
    "    print(f\"{data_dir} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d0fd02-5e52-4c8d-ae16-5ca40bab3072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6d0fd02-5e52-4c8d-ae16-5ca40bab3072",
    "outputId": "a9214941-ef38-45ef-ad26-75b5b3ad28c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading janlpbook.s3.amazonaws.com/sentences20210924.tar.bz2 to chp05_03/data/sentences20210924.tar.bz2\n",
      "Done.\n",
      "--------------------------------------------------\n",
      "Extracting file chp05_03/data/sentences20210924.tar.bz2\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Download and prep the Tatoeba datasets\n",
    "if not data_src.is_file():\n",
    "    print(f\"Downloading {data_url} to {data_src}\")\n",
    "    subprocess.run(shlex.split(f\"wget -q -O {data_src} {data_url}\"))\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"{data_src} exists.\")\n",
    "\n",
    "HR()\n",
    "\n",
    "if not data_path.is_file():\n",
    "    print(f\"Extracting file {data_src}\")\n",
    "    shutil.unpack_archive(data_src, data_dir)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"{data_path} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65704b44-8a30-4496-b563-f48dd28d231e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65704b44-8a30-4496-b563-f48dd28d231e",
    "outputId": "f2339adb-e270-4394-c8f8-45cd65566d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading janlpbook.s3.amazonaws.com/cc100.ja.mod1k.txt to chp05_03/data/cc100.ja.mod1k.txt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Download the CC-100 datasets\n",
    "if not data_path2.is_file():\n",
    "    print(f\"Downloading {data_url2} to {data_path2:}\")\n",
    "    subprocess.run(shlex.split(f\"wget -q -O {data_path2:} {data_url2}\"))\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"{data_path2} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ffe8d8-3ab5-43d9-aaad-92eac870fbfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39ffe8d8-3ab5-43d9-aaad-92eac870fbfe",
    "outputId": "3ef309e2-26e0-4725-a1ad-4cf93021f872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tcmn\t我們試試看！\n",
      "2\tcmn\t我该去睡觉了。\n",
      "3\tcmn\t你在干什麼啊？\n",
      "4\tcmn\t這是什麼啊？\n",
      "5\tcmn\t今天是６月１８号，也是Muiriel的生日！\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "!head -n 5 {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734a6baa-73cb-424c-9151-3edf5f4bd373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "734a6baa-73cb-424c-9151-3edf5f4bd373",
    "outputId": "20c7bfa7-9224-467e-935e-93a0a67a64a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chp05_03/data/sentences.jpn')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file3 = \"sentences.jpn\"\n",
    "data_path3 = data_dir / data_file3\n",
    "data_path3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hhOiI9GBaRre",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhOiI9GBaRre",
    "outputId": "4c303b68-f87f-4bf6-e63b-06cef76814e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ack compatibility with --perl-regexp on both OSX and Linux:\n",
      "きみにちょっとしたものをもってきたよ。\n"
     ]
    }
   ],
   "source": [
    "# OSX: grep changed from grep (GNU grep) 2.5.1 in 10.7 to grep 2.5.1-FreeBSD in OSX 10.8\n",
    "# Accordingly, the FreeBSD grep version no longer supports -P, --perl-regexp\n",
    "# Instead, we use 'ack' to ensure cross-compatibility across OSX and Linux.\n",
    "\n",
    "print(\"Test ack compatibility with --perl-regexp on both OSX and Linux:\")\n",
    "ack_test = !ack -1 '\\tjpn\\t' chp05_03/data/sentences.csv | cut -f 3\n",
    "print(ack_test[0])\n",
    "assert ack_test[0] == \"きみにちょっとしたものをもってきたよ。\", \"Problem with ack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc8472d-8afd-4dc3-9231-c441280a77e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fc8472d-8afd-4dc3-9231-c441280a77e6",
    "outputId": "fb44f581-a2f0-4469-8b77-63ac98c4122c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chp05_03/data/sentences.jpn\n",
      "--------------------------------------------------\n",
      "きみにちょっとしたものをもってきたよ。\n",
      "何かしてみましょう。\n",
      "私は眠らなければなりません。\n",
      "何してるの？\n",
      "今日は６月１８日で、ムーリエルの誕生日です！\n",
      "--------------------------------------------------\n",
      "12M\tchp05_03/data/sentences.jpn\n"
     ]
    }
   ],
   "source": [
    "# Remove extra fields from sentences.csv\n",
    "# Without `cut field 3`, the output is \"1297 jpn きみにちょっとしたものをもってきたよ。\"\n",
    "# With `cut field 3`, the output is \"きみにちょっとしたものをもってきたよ。\"\n",
    "if not Path(data_dir/\"sentences.jpn\").is_file():\n",
    "    print(f\"Creating {data_dir}/sentences.jpn\")\n",
    "    !ack '\\tjpn\\t' {data_dir}/sentences.csv | cut -f 3 > {data_dir}/sentences.jpn\n",
    "else:\n",
    "    print(f\"{data_dir}/sentences.jpn exists.\")\n",
    "\n",
    "HR()\n",
    "\n",
    "!head -n 5 {data_dir}/sentences.jpn\n",
    "HR()\n",
    "!du -h {data_dir}/sentences.jpn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde496b-5249-449a-9838-915035f23f81",
   "metadata": {
    "id": "4cde496b-5249-449a-9838-915035f23f81"
   },
   "source": [
    "* Open these two files and build two files, a Kanji-Kana majiribun file with the original text, and a Romaji file with the converted text from cutlet.\n",
    "\n",
    "* Use `itertools.chain()` from https://docs.python.org/3/library/itertools.html#itertools.chain. Make an iterator that returns elements from the first iterable until it is exhausted, then proceeds to the next iterable, until all of the iterables are exhausted. Used for treating consecutive sequences as a single sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9aa98-37ae-49b6-bed5-f8ef4fdc31cb",
   "metadata": {
    "id": "d1d9aa98-37ae-49b6-bed5-f8ef4fdc31cb"
   },
   "source": [
    "## Error: \n",
    "\n",
    "* Looks like `cutlet.Cutlet().romaji()` is choking on this text in cc100.ja.mod1k.txt, line 446182:\n",
    "    \n",
    "```\n",
    "ｺﾝﾊﾞﾝﾊｰヾ(･∀･`o)ﾉ))明細書については、今日中に一通り終わらせないと先行きが不安ですが、普通にムリだろうなと諦めています。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106d38af-03fd-4818-95ca-7dbb98567b9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "106d38af-03fd-4818-95ca-7dbb98567b9b",
    "outputId": "e6c2fc98-1f53-4fc8-94c1-89e7f6734b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ｺﾝﾊﾞﾝﾊｰヾ(･∀･`o)ﾉ))明細書については、今日中に一通り終わらせないと先行きが不安ですが、普通にムリだろうなと諦めています。\n",
      "--------------------------------------------------\n",
      "Error: substring not found\n",
      "--------------------------------------------------\n",
      "ｺﾝﾊﾞﾝﾊｰ(･∀･`o)ﾉ))明細書については、今日中に一通り終わらせないと先行きが不安ですが、普通にムリだろうなと諦めています。\n",
      "--------------------------------------------------\n",
      "Konbanhaa  )) meisaisho ni tsuite wa, kyoujuu ni hitotoori owarasenaito sakiyuki ga fuan desuga, futsuu ni muri darou na to akiramete imasu.\n"
     ]
    }
   ],
   "source": [
    "def test_odoriji():\n",
    "    # cc100.ja.mod1k.txt, line 446182\n",
    "    input = \"ｺﾝﾊﾞﾝﾊｰヾ(･∀･`o)ﾉ))明細書については、今日中に一通り終わらせないと先行きが不安ですが、普通にムリだろうなと諦めています。\"\n",
    "    print(input)\n",
    "    HR()\n",
    "    \n",
    "    try:\n",
    "        print(cutlet.Cutlet().romaji(input))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    HR()\n",
    "    \n",
    "    # Hack to replace the odoriji ヾ.\n",
    "    # The other odoriji ゝゞヽ do not seem to causes any errors.\n",
    "    input = input.replace(\"ヾ\", \"\")\n",
    "    print(input)\n",
    "    HR()\n",
    "    \n",
    "    try:\n",
    "        print(cutlet.Cutlet().romaji(input))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "test_odoriji()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636ef388-fce2-4113-9b96-80f10c8e711d",
   "metadata": {
    "id": "636ef388-fce2-4113-9b96-80f10c8e711d"
   },
   "outputs": [],
   "source": [
    "def line_count(filename):\n",
    "    # Purposely use subprocess for non-async\n",
    "    return int(subprocess.check_output(['wc', '-l', filename]).split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c017cb-8555-4616-9276-ee9d3d7842b6",
   "metadata": {
    "id": "d8c017cb-8555-4616-9276-ee9d3d7842b6"
   },
   "source": [
    "* GB: For consistency, rename filenames to use underbars as `tatoeba_cc100.kan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8448da89-28b4-4457-82fd-2a3bf492ae66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8448da89-28b4-4457-82fd-2a3bf492ae66",
    "outputId": "ed911e26-806a-44c3-df2a-8c0128da39e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 671,370\n"
     ]
    }
   ],
   "source": [
    "fin1_n = line_count(f\"{data_dir}/sentences.jpn\")\n",
    "fin2_n = line_count(f\"{data_dir}/cc100.ja.mod1k.txt\")\n",
    "fin_chain_n = fin1_n + fin2_n\n",
    "print(f\"Total lines: {fin_chain_n:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a64112-c9a3-4b18-8dab-cd9273bc7304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55a64112-c9a3-4b18-8dab-cd9273bc7304",
    "outputId": "bb4749b3-7f80-462a-d675-eb9d51121bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chp05_03/data/raw_text\n"
     ]
    }
   ],
   "source": [
    "raw_text = data_dir / \"raw_text\"\n",
    "\n",
    "if not raw_text.is_dir():\n",
    "    print(f\"Creating {raw_text}\")\n",
    "    raw_text.mkdir()\n",
    "else:\n",
    "    print(f\"{raw_text} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f550268a-2335-431f-b308-e250fdefdc23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f550268a-2335-431f-b308-e250fdefdc23",
    "outputId": "8060ce17-647e-4046-ade6-42ef6ac7124b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 671370/671370 [03:46<00:00, 2960.62it/s]\n"
     ]
    }
   ],
   "source": [
    "if not (Path(f\"{raw_text}/tatoeba_cc100.kan\").is_file() and Path(f\"{raw_text}/tatoeba_cc100.rom\").is_file()):\n",
    "    \n",
    "    with open(f\"{data_dir}/sentences.jpn\") as fin1, \\\n",
    "        open(f\"{data_dir}/cc100.ja.mod1k.txt\") as fin2, \\\n",
    "        open(f\"{raw_text}/tatoeba_cc100.kan\", mode='w') as f_kan, \\\n",
    "        open(f\"{raw_text}/tatoeba_cc100.rom\", mode='w') as f_rom:\n",
    "\n",
    "        for line in tqdm(chain(fin1, fin2), total=fin_chain_n):\n",
    "            sent_kan = line.strip()\n",
    "            if not sent_kan:\n",
    "                continue\n",
    "            if len(sent_kan) > 256:\n",
    "                # skip long sentences\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Hack to remove odoriji ヾ, otherwise it crashes cutlet.Cutlet().romaji()\n",
    "                # str.replace() should be fast enough here.\n",
    "                sent_kan = sent_kan.replace(\"ヾ\", \"\")             \n",
    "                sent_rom = katsu.romaji(sent_kan)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            sent_rom = sent_rom.replace(' ', '') \n",
    "            f_kan.write(sent_kan + '\\n')\n",
    "            f_rom.write(sent_rom + '\\n')\n",
    "            \n",
    "else:\n",
    "    print(f\"{raw_text}/tatoeba_cc100.rom exists.\")\n",
    "    print(f\"{raw_text}/tatoeba_cc100.kan exists.\")                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e3a4fb8-788a-4fb9-b052-68397895c9d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e3a4fb8-788a-4fb9-b052-68397895c9d4",
    "outputId": "5cfe02c1-765d-4fd2-df49-250e73c0321f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> chp05_03/data/raw_text/tatoeba_cc100.kan <==\n",
      "きみにちょっとしたものをもってきたよ。\n",
      "何かしてみましょう。\n",
      "私は眠らなければなりません。\n",
      "何してるの？\n",
      "今日は６月１８日で、ムーリエルの誕生日です！\n",
      "お誕生日おめでとうムーリエル！\n",
      "ムーリエルは２０歳になりました。\n",
      "パスワードは「Muiriel」です。\n",
      "すぐに戻ります。\n",
      "知らない。\n",
      "\n",
      "==> chp05_03/data/raw_text/tatoeba_cc100.rom <==\n",
      "Kiminichottoshitamonowomottekitayo.\n",
      "Nankashitemimashou.\n",
      "Watakushiwanemuranakerebanarimasen.\n",
      "Nanshiteruno?\n",
      "Kyouwa6tsuki18kade,Muurierunotanjouhidesu!\n",
      "OtanjouhiomedetouMuurieru!\n",
      "Muurieruwa20saininarimashita.\n",
      "Pasuwaadowa\"Muiriel\"desu.\n",
      "Sugunimodorimasu.\n",
      "Shiranai.\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 {raw_text}/tatoeba_cc100.kan {raw_text}/tatoeba_cc100.rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2DhmYKAfClmN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DhmYKAfClmN",
    "outputId": "3565d7b2-a8ea-4591-e5b9-c703cd5a4d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ｺﾝﾊﾞﾝﾊｰ(･∀･`o)ﾉ))明細書については、今日中に一通り終わらせないと先行きが不安ですが、普通にムリだろうなと諦めています。 葛西りいちさんの作品をできれば電子書籍で読みたいんです。スキャナで自炊するのは面倒 … [Read more…]\n",
      "Konbanhaa))meisaishonitsuitewa,kyoujuunihitotooriowarasenaitosakiyukigafuandesuga,futsuunimuridarounatoakirameteimasu.KasaiRiichisannosakuhinwodekirebadenshishosekideyomitaindesu.sukyanadejisuisurunowamendou...[Readmore...]\n"
     ]
    }
   ],
   "source": [
    "# Check the result of replacing odoriji ヾ. Note that hankaku-katakana still remains.\n",
    "odoriji_check = !grep -n --binary-files=text 明細書については {raw_text}/tatoeba_cc100.kan | cut -d ':' -f1\n",
    "!sed -n '{odoriji_check[0]}p' {raw_text}/tatoeba_cc100.kan\n",
    "!sed -n '{odoriji_check[0]}p' {raw_text}/tatoeba_cc100.rom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fb05f-4940-4643-afd8-3b62066f5a64",
   "metadata": {
    "id": "de0fb05f-4940-4643-afd8-3b62066f5a64"
   },
   "source": [
    "<a name='5.3.3'></a><a id='5.3.3'></a>\n",
    "## 5.3.3 Training and Tokenizing with SentencePiece (subword tokenization)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We are using the SentencePiece Python Wrapper.\n",
    "\n",
    "Training is performed by passing parameters of spm_train to SentencePieceTrainer.train() function.\n",
    "\n",
    "Training API (quick example)\n",
    "\n",
    "```\n",
    "% spm_train --input=<input> --model_prefix=<model_name> --vocab_size=8000 --character_coverage=1.0 --model_type=<type>\n",
    "```\n",
    "* `--input`: one-sentence-per-line **raw** corpus file. No need to run\n",
    "  tokenizer, normalizer or preprocessor. By default, SentencePiece normalizes\n",
    "  the input with Unicode NFKC. You can pass a comma-separated list of files.\n",
    "* `--model_prefix`: output model name prefix. `<model_name>.model` and `<model_name>.vocab` are generated.\n",
    "* `--vocab_size`: vocabulary size, e.g., 8000, 16000, or 32000\n",
    "* `--character_coverage`: amount of characters covered by the model, good defaults are: `0.9995` for languages with rich character set like Japanese or Chinese and `1.0` for other languages with small character set.\n",
    "* `--model_type`: model type. Choose from `unigram` (default), `bpe`, `char`, or `word`. The input sentence must be pretokenized when using `word` type.\n",
    "\n",
    "Resources:\n",
    "\n",
    "* https://github.com/google/sentencepiece\n",
    "* https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "* https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "* https://github.com/google/sentencepiece/blob/master/doc/options.md#training-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tjWj6bew9wXH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjWj6bew9wXH",
    "outputId": "a536ccae-f0cd-4a3b-c98d-4943702bd6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "きみにちょっとしたものをもってきたよ。\n",
      "何かしてみましょう。\n",
      "私は眠らなければなりません。\n",
      "何してるの？\n",
      "今日は６月１８日で、ムーリエルの誕生日です！\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 {raw_text}/tatoeba_cc100.kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94fab48-2bad-4c4e-b876-c99dcc22aeaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d94fab48-2bad-4c4e-b876-c99dcc22aeaf",
    "outputId": "6dd0cfb0-a1ea-435a-defb-31026fc76dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SentencePiece models in chp05_03/data/sp_tokenizer_models.\n"
     ]
    }
   ],
   "source": [
    "sp_tokenizer_models = data_dir / \"sp_tokenizer_models\"\n",
    "\n",
    "if not sp_tokenizer_models.is_dir():\n",
    "    print(f\"Creating SentencePiece models in {sp_tokenizer_models}.\")\n",
    "    sp_tokenizer_models.mkdir()\n",
    "else:\n",
    "    print(f\"{sp_tokenizer_models} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "KXS3EsnaEf0w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXS3EsnaEf0w",
    "outputId": "57505a68-60a0-42c1-fbe6-0330c43d73e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SentencePiece tokenizer models for kanji data in chp05_03/data/sp_tokenizer_models.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if not Path(sp_tokenizer_models / \"tatoeba_cc100.kan.spm.model\").is_file():\n",
    "    \n",
    "    print(f\"Create SentencePiece tokenizer models for kanji data in {sp_tokenizer_models}.\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=f'{raw_text}/tatoeba_cc100.kan',\n",
    "        model_prefix=f'{sp_tokenizer_models}/tatoeba_cc100.kan.spm',\n",
    "        vocab_size=10_000,\n",
    "        input_sentence_size=100_000,\n",
    "        shuffle_input_sentence=True,\n",
    "        minloglevel=1\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"File {sp_tokenizer_models}/tatoeba_cc100.kan.spm.model exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8a8d11f-283b-4ecc-98a9-319a01c9fba1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8a8d11f-283b-4ecc-98a9-319a01c9fba1",
    "outputId": "3137f467-2236-4e0e-909e-a57c6a089d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SentencePiece tokenizer models for romaji data in chp05_03/data/sp_tokenizer_models.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if not Path(sp_tokenizer_models / \"tatoeba_cc100.rom.spm.model\").is_file():\n",
    "    \n",
    "    print(f\"Create SentencePiece tokenizer models for romaji data in {sp_tokenizer_models}.\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=f'{raw_text}/tatoeba_cc100.rom',\n",
    "        model_prefix=f'{sp_tokenizer_models}/tatoeba_cc100.rom.spm',\n",
    "        vocab_size=1_000,\n",
    "        input_sentence_size=100_000,\n",
    "        shuffle_input_sentence=True,\n",
    "        minloglevel=1\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"File {sp_tokenizer_models}/tatoeba_cc100.rom.spm.model exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf571f-abeb-4b0b-8105-d36d2a52b914",
   "metadata": {
    "id": "b3cf571f-abeb-4b0b-8105-d36d2a52b914"
   },
   "source": [
    "---\n",
    "* Test the trained tokenization models on a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63493404-540a-4a6b-b409-1956a0ed5812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63493404-540a-4a6b-b409-1956a0ed5812",
    "outputId": "d12a242f-15fd-4d2c-b371-e7daef0a85fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f7f0965fd80> >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Makes segmenter instance and loads the model file\n",
    "sp_kan = spm.SentencePieceProcessor(\n",
    "    model_file=f\"{sp_tokenizer_models}/tatoeba_cc100.kan.spm.model\"\n",
    ")\n",
    "sp_kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ATIwUq4FRh27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATIwUq4FRh27",
    "outputId": "b9a4bf2d-6f34-40ab-8e9c-cfc5a9741a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁これは', 'テスト', 'です', '。']\n"
     ]
    }
   ],
   "source": [
    "# id <=> piece conversion\n",
    "print(sp_kan.id_to_piece(sp_kan.encode(\"これはテストです。\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "YTQWs2vD_Ba2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTQWs2vD_Ba2",
    "outputId": "89ef3502-887a-48a4-cea8-72c774002a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<unk>', '魅', '<unk>', 'が', '<unk>', 'する']\n"
     ]
    }
   ],
   "source": [
    "# This causes more problems for the tokenizer\n",
    "print(sp_kan.id_to_piece(sp_kan.encode(\"魑魅魍魎が跋扈する\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd741c1e-8780-42bf-8275-d654448702b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd741c1e-8780-42bf-8275-d654448702b7",
    "outputId": "4c42b64c-75bd-4c9f-d5c8-a507314c3ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Kore', 'wa', 'te', 'suto', 'desu', '.']\n"
     ]
    }
   ],
   "source": [
    "sp_rom = spm.SentencePieceProcessor(\n",
    "    model_file=f\"{sp_tokenizer_models}/tatoeba_cc100.rom.spm.model\"\n",
    ")\n",
    "\n",
    "print(sp_rom.id_to_piece(sp_rom.encode(\"Korewatesutodesu.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b3d086f-baa4-482d-9689-bdef810ae2c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b3d086f-baa4-482d-9689-bdef810ae2c2",
    "outputId": "912ceae0-e5b6-483c-945e-29f92790e5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the entire corpus in chp05_03/data/tokenized_corpus.\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = data_dir / \"tokenized_corpus\"\n",
    "\n",
    "if not tokenized_corpus.is_dir():\n",
    "    print(f\"Tokenize the entire corpus in {tokenized_corpus}.\")\n",
    "    tokenized_corpus.mkdir()\n",
    "else:\n",
    "    print(f\"{tokenized_corpus} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee2837d1-b8c9-41be-9359-b31415b39733",
   "metadata": {
    "id": "ee2837d1-b8c9-41be-9359-b31415b39733"
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus_fn(spp, file_source, file_target):\n",
    "    fin_n = line_count(file_source)\n",
    "    print(f\"Writing to {file_target}\")\n",
    "\n",
    "    with open(file_source) as fin, open(file_target, mode=\"w\") as fout:\n",
    "        for line in tqdm(fin, total=fin_n):\n",
    "            sent_jpn = line.strip()\n",
    "            tokens = spp.id_to_piece(spp.encode(sent_jpn))\n",
    "            # Join all items into a single string, with space character as separator.\n",
    "            fout.write(\" \".join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70c3c836-3c5d-455f-a21e-35146728534d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70c3c836-3c5d-455f-a21e-35146728534d",
    "outputId": "566132c4-8bd3-4ed4-f71b-74d60278582e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.kan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 603144/603144 [00:25<00:00, 23431.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the kanji corpus\n",
    "file_source_kan = f\"{raw_text}/tatoeba_cc100.kan\"\n",
    "file_target_kan = f\"{tokenized_corpus}/tatoeba_cc100.tok.kan\"\n",
    "\n",
    "if not Path(f\"{tokenized_corpus}/tatoeba_cc100.tok.kan\").is_file():\n",
    "    tokenize_corpus_fn(sp_kan, file_source_kan, file_target_kan)\n",
    "else:\n",
    "    print(f\"{tokenized_corpus}/tatoeba_cc100.tok.kan exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2f2f6b4-79df-4943-ac6c-71754417ef9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2f2f6b4-79df-4943-ac6c-71754417ef9b",
    "outputId": "8c0dab00-e74a-48ad-a821-6a70aa1e8f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.rom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 603144/603144 [00:29<00:00, 20341.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the romaji corpus\n",
    "file_source_rom = f\"{raw_text}/tatoeba_cc100.rom\"\n",
    "file_target_rom = f\"{tokenized_corpus}/tatoeba_cc100.tok.rom\"\n",
    "\n",
    "if not Path(f\"{tokenized_corpus}/tatoeba_cc100.tok.rom\").is_file():\n",
    "    tokenize_corpus_fn(sp_rom, file_source_rom, file_target_rom)\n",
    "else:\n",
    "    print(f\"{tokenized_corpus}/tatoeba_cc100.tok.rom exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1131ba90-53a0-4770-8599-94db1faeece4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1131ba90-53a0-4770-8599-94db1faeece4",
    "outputId": "5e91701d-ac5b-413c-c1ea-d117f141f0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.kan <==\n",
      "▁ き み に ちょっとした ものを も ってきた よ 。\n",
      "▁何か してみましょう 。\n",
      "▁私は 眠 ら なければなりません 。\n",
      "▁何 してる の ?\n",
      "▁今日は 6 月 18 日 で 、 ム ー リ エル の 誕生日 です !\n",
      "▁お 誕生日 お め で と う ム ー リ エル !\n",
      "▁ ム ー リ エル は 20 歳 になりました 。\n",
      "▁ パスワード は 「 M u i ri el 」 です 。\n",
      "▁ すぐに 戻り ます 。\n",
      "▁ 知らない 。\n",
      "\n",
      "==> chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.rom <==\n",
      "▁Kimi ni chotto shita monowo mo ttekita yo .\n",
      "▁Nan ka shite mi mashou .\n",
      "▁Watakushiwa ne mu ra nakereba n arimasen .\n",
      "▁Nan shite runo ?\n",
      "▁Kyou wa 6 tsuki 18 ka de , M u u ri eru no tan jou hi desu !\n",
      "▁O tan jou hi o me de tou M u u ri eru !\n",
      "▁Mu u ri eru wa 20 sai ninarimashita .\n",
      "▁ P a su wa a do wa \" M u i ri e l \" desu .\n",
      "▁Su gu nimo do rimasu .\n",
      "▁Shi ranai .\n"
     ]
    }
   ],
   "source": [
    "# Examine the first 10 lines from each file\n",
    "!head -n 10 {tokenized_corpus}/tatoeba_cc100.tok.kan {tokenized_corpus}/tatoeba_cc100.tok.rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "519df78f-a675-42fb-8c8f-5747b2c88266",
   "metadata": {
    "id": "519df78f-a675-42fb-8c8f-5747b2c88266"
   },
   "outputs": [],
   "source": [
    "def create_validation_set(file, target):\n",
    "    if not Path(tokenized_corpus / file).is_file():\n",
    "        print(\"Create validation dataset.\")\n",
    "        # NR stands for Number Record\n",
    "        # Get only every 100th line\n",
    "        !awk 'NR%100==0' {target} > {tokenized_corpus}/{file}\n",
    "    else:\n",
    "        print(f\"{tokenized_corpus}/{file} exists.\")\n",
    "\n",
    "    print(f\"line count: {line_count(f'{tokenized_corpus}/{file}'):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "820b152f-f256-4bbb-8c63-0b342efce34b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "820b152f-f256-4bbb-8c63-0b342efce34b",
    "outputId": "669951db-620e-4b1d-88d5-fb4889441719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create validation dataset.\n",
      "line count: 6,031\n"
     ]
    }
   ],
   "source": [
    "# Create Kanji validation set\n",
    "create_validation_set(\"tatoeba_cc100.tok.valid.kan\", file_target_kan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d7b0d2d-ec69-40ee-9e9a-06c967a79b40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d7b0d2d-ec69-40ee-9e9a-06c967a79b40",
    "outputId": "f49ea1d6-0184-44dc-9d9f-f19ac513db40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create validation dataset.\n",
      "line count: 6,031\n"
     ]
    }
   ],
   "source": [
    "# Create Romaji validation set\n",
    "create_validation_set(\"tatoeba_cc100.tok.valid.rom\", file_target_rom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "553eee7a-7ba0-437c-8735-30216a76c673",
   "metadata": {
    "id": "553eee7a-7ba0-437c-8735-30216a76c673"
   },
   "outputs": [],
   "source": [
    "def create_training_set(file, target):\n",
    "    if not Path(tokenized_corpus / file).is_file():\n",
    "        print(\"Create training dataset.\")\n",
    "        # NR stands for Number Record\n",
    "        # Get every line except every 100th\n",
    "        !awk 'NR%100!=0' {target} > {tokenized_corpus}/{file}\n",
    "    else:\n",
    "        print(f\"{tokenized_corpus}/{file} exists.\")\n",
    "\n",
    "    print(f\"line count: {line_count(f'{tokenized_corpus}/{file}'):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63838766-56b7-4a6b-a62a-11b564f38555",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63838766-56b7-4a6b-a62a-11b564f38555",
    "outputId": "98844cfa-2d9e-41bc-8ef1-81900f610bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training dataset.\n",
      "line count: 597,113\n"
     ]
    }
   ],
   "source": [
    "# Create Kanji training set\n",
    "create_training_set(\"tatoeba_cc100.tok.train.kan\", file_target_kan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c899b38-0a29-4332-a51e-94927e95ac2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c899b38-0a29-4332-a51e-94927e95ac2a",
    "outputId": "88a65f88-29b5-4e6a-9757-8393bf5cd04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create training dataset.\n",
      "line count: 597,113\n"
     ]
    }
   ],
   "source": [
    "# Create Romaji training set\n",
    "create_training_set(\"tatoeba_cc100.tok.train.rom\", file_target_rom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e52e2a-5a4c-42b3-9b1d-fdbf361384d9",
   "metadata": {
    "id": "d3e52e2a-5a4c-42b3-9b1d-fdbf361384d9"
   },
   "source": [
    "<a name='5.3.4'></a><a id='5.3.4'></a>\n",
    "## 5.3.4 Training a Conversion Model with Fairseq\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* Fairseq is a sequence modeling toolkit by Meta AI\n",
    "* It implements major sequence models such as the Transformer-based seq2seq model.\n",
    "* We need to first convert the raw text corpus into a binary format.\n",
    "* This creates binary files for each language and data split.\n",
    "\n",
    "Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.\n",
    "\n",
    "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs\n",
    "\n",
    "Reference:\n",
    "\n",
    "* https://github.com/facebookresearch/fairseq\n",
    "* https://aclanthology.org/N19-4009.pdf\n",
    "* https://www.youtube.com/watch?v=OtgDdWtHvto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "157aac0c-a8d4-4ec9-af10-484ab02dba1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "157aac0c-a8d4-4ec9-af10-484ab02dba1f",
    "outputId": "c07d8510-24e8-44bb-aace-66bfa3ad0b03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chp05_03/data/bin')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin = data_dir / \"bin\"\n",
    "bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bf886bf-a517-4f0b-89f0-67072411a941",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bf886bf-a517-4f0b-89f0-67072411a941",
    "outputId": "ab383486-4e8d-4f5c-c864-285b18e71ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting raw text corpus to a binary format.\n",
      "--------------------------------------------------\n",
      "2022-12-07 10:36:16 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='chp05_03/data/bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='rom', srcdict=None, suppress_crashes=False, target_lang='kan', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.train', use_plasma_view=False, user_dir=None, validpref='chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.valid', wandb_project=None, workers=4)\n",
      "2022-12-07 10:37:28 | INFO | fairseq_cli.preprocess | [rom] Dictionary: 1008 types\n",
      "2022-12-07 10:39:01 | INFO | fairseq_cli.preprocess | [rom] chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.train.rom: 597113 sents, 21224028 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-07 10:39:01 | INFO | fairseq_cli.preprocess | [rom] Dictionary: 1008 types\n",
      "2022-12-07 10:39:02 | INFO | fairseq_cli.preprocess | [rom] chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.valid.rom: 6031 sents, 214974 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-07 10:39:02 | INFO | fairseq_cli.preprocess | [kan] Dictionary: 10000 types\n",
      "2022-12-07 10:40:38 | INFO | fairseq_cli.preprocess | [kan] chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.train.kan: 597113 sents, 15558841 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-07 10:40:38 | INFO | fairseq_cli.preprocess | [kan] Dictionary: 10000 types\n",
      "2022-12-07 10:40:39 | INFO | fairseq_cli.preprocess | [kan] chp05_03/data/tokenized_corpus/tatoeba_cc100.tok.valid.kan: 6031 sents, 157550 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-07 10:40:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to chp05_03/data/bin\n",
      "--------------------------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# rom: Romaji\n",
    "# kan: Kanji\n",
    "if not bin.is_dir():\n",
    "    print(\"Converting raw text corpus to a binary format.\")\n",
    "    HR() \n",
    "    !fairseq-preprocess --source-lang rom --target-lang kan \\\n",
    "        --trainpref {tokenized_corpus}/tatoeba_cc100.tok.train \\\n",
    "        --validpref {tokenized_corpus}/tatoeba_cc100.tok.valid \\\n",
    "        --destdir {bin} \\\n",
    "        --workers 4\n",
    "    HR()\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(f\"{bin} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c82f7-ad44-452e-9fa5-9cb05cbd5126",
   "metadata": {
    "id": "485c82f7-ad44-452e-9fa5-9cb05cbd5126"
   },
   "source": [
    "* Run fairseq-train to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c5204d9-4b92-4cbe-8d8b-8be58ff993db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c5204d9-4b92-4cbe-8d8b-8be58ff993db",
    "outputId": "504ee4d2-12af-4703-d4ec-4dc7bc9e95d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc3b1a0a-d8e4-4c7d-a7ac-552a142d3dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc3b1a0a-d8e4-4c7d-a7ac-552a142d3dbd",
    "outputId": "10d2d7d6-0b17-4705-ee79-10d6b415fb7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('chp05_03/data/models')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = data_dir / \"models\"\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39d5094c-9d7f-49ca-8ad4-d7d53c32eca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d5094c-9d7f-49ca-8ad4-d7d53c32eca6",
    "outputId": "6373b82b-e726-471a-f86d-dc0f1926a9dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with fairseq-train on COLAB\n",
      "--------------------------------------------------\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 16384, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 16384, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'chp05_03/data/models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='chp05_03/data/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=2, max_tokens=16384, max_tokens_valid=16384, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='chp05_03/data/models', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'chp05_03/data/bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-12-07 10:55:25 | INFO | fairseq.tasks.translation | [rom] dictionary: 1008 types\n",
      "2022-12-07 10:55:25 | INFO | fairseq.tasks.translation | [kan] dictionary: 10000 types\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(1008, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(10000, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=10000, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | num. shared model params: 52,783,104 (num. trained: 52,783,104)\n",
      "2022-12-07 10:55:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-12-07 10:55:25 | INFO | fairseq.data.data_utils | loaded 6,031 examples from: chp05_03/data/bin/valid.rom-kan.rom\n",
      "2022-12-07 10:55:26 | INFO | fairseq.data.data_utils | loaded 6,031 examples from: chp05_03/data/bin/valid.rom-kan.kan\n",
      "2022-12-07 10:55:26 | INFO | fairseq.tasks.translation | chp05_03/data/bin valid rom-kan 6031 examples\n",
      "2022-12-07 10:55:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-12-07 10:55:27 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
      "2022-12-07 10:55:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-12-07 10:55:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-12-07 10:55:27 | INFO | fairseq_cli.train | max tokens per device = 16384 and max sentences per device = None\n",
      "2022-12-07 10:55:27 | INFO | fairseq.trainer | Preparing to load checkpoint chp05_03/data/models/checkpoint_last.pt\n",
      "2022-12-07 10:55:27 | INFO | fairseq.trainer | No existing checkpoint found chp05_03/data/models/checkpoint_last.pt\n",
      "2022-12-07 10:55:27 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-12-07 10:55:27 | INFO | fairseq.data.data_utils | loaded 597,113 examples from: chp05_03/data/bin/train.rom-kan.rom\n",
      "2022-12-07 10:55:27 | INFO | fairseq.data.data_utils | loaded 597,113 examples from: chp05_03/data/bin/train.rom-kan.kan\n",
      "2022-12-07 10:55:27 | INFO | fairseq.tasks.translation | chp05_03/data/bin train rom-kan 597113 examples\n",
      "2022-12-07 10:55:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1342\n",
      "epoch 001:   0% 0/1342 [00:00<?, ?it/s]2022-12-07 10:55:28 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-12-07 10:55:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2022-12-07 10:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0% 1/1342 [00:01<29:34,  1.32s/it]2022-12-07 10:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   1% 7/1342 [00:03<07:33,  2.94it/s]2022-12-07 10:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   4% 55/1342 [00:16<05:51,  3.66it/s]2022-12-07 10:55:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100% 1341/1342 [06:58<00:00,  3.23it/s, loss=8.709, ppl=418.59, wps=36819.9, ups=3.18, wpb=11582.1, bsz=388, num_updates=1300, lr=6.5e-05, gnorm=2.369, loss_scale=8, train_wall=31, gb_free=10.4, wall=407]2022-12-07 11:02:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   6% 1/17 [00:00<00:03,  4.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  18% 3/17 [00:00<00:01,  7.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24% 4/17 [00:00<00:01,  8.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29% 5/17 [00:00<00:01,  8.38it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41% 7/17 [00:00<00:01,  9.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  47% 8/17 [00:00<00:00,  9.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  53% 9/17 [00:01<00:00,  8.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  59% 10/17 [00:01<00:00,  8.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  65% 11/17 [00:01<00:00,  8.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71% 12/17 [00:01<00:00,  8.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76% 13/17 [00:01<00:00,  8.99it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82% 14/17 [00:01<00:00,  8.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88% 15/17 [00:01<00:00,  8.67it/s]\u001b[A\n",
      "                                                                        \u001b[A2022-12-07 11:02:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.056 | ppl 266.09 | wps 92056.9 | wpb 9267.6 | bsz 354.8 | num_updates 1338\n",
      "2022-12-07 11:02:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1338 updates\n",
      "2022-12-07 11:02:29 | INFO | fairseq.trainer | Saving checkpoint to /content/chp05_03/data/models/checkpoint_best.pt\n",
      "2022-12-07 11:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/chp05_03/data/models/checkpoint_best.pt\n",
      "2022-12-07 11:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint chp05_03/data/models/checkpoint_best.pt (epoch 1 @ 1338 updates, score 8.056) (writing took 5.0937126419999 seconds)\n",
      "2022-12-07 11:02:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-12-07 11:02:34 | INFO | train | epoch 001 | loss 10.101 | ppl 1098.58 | wps 36550.1 | ups 3.15 | wpb 11593 | bsz 443.7 | num_updates 1338 | lr 6.69e-05 | gnorm 2.243 | loss_scale 8 | train_wall 416 | gb_free 10.5 | wall 426\n",
      "2022-12-07 11:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1342\n",
      "epoch 002:   0% 0/1342 [00:00<?, ?it/s]2022-12-07 11:02:34 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-12-07 11:02:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100% 1341/1342 [07:02<00:00,  3.34it/s, loss=4.044, ppl=16.49, wps=36264.4, ups=3.17, wpb=11431.4, bsz=474.2, num_updates=2600, lr=0.00013, gnorm=4.141, loss_scale=8, train_wall=31, gb_free=10.8, wall=824]2022-12-07 11:09:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   6% 1/17 [00:00<00:02,  5.85it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  18% 3/17 [00:00<00:01,  8.30it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  24% 4/17 [00:00<00:01,  8.70it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  29% 5/17 [00:00<00:01,  8.88it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  41% 7/17 [00:00<00:01,  9.47it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  53% 9/17 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  59% 10/17 [00:01<00:00,  9.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  65% 11/17 [00:01<00:00,  9.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71% 12/17 [00:01<00:00,  8.95it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  82% 14/17 [00:01<00:00,  8.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  88% 15/17 [00:01<00:00,  8.87it/s]\u001b[A\n",
      "                                                                        \u001b[A2022-12-07 11:09:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.257 | ppl 9.56 | wps 92320 | wpb 9267.6 | bsz 354.8 | num_updates 2680 | best_loss 3.257\n",
      "2022-12-07 11:09:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2680 updates\n",
      "2022-12-07 11:09:38 | INFO | fairseq.trainer | Saving checkpoint to /content/chp05_03/data/models/checkpoint_best.pt\n",
      "2022-12-07 11:09:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/chp05_03/data/models/checkpoint_best.pt\n",
      "2022-12-07 11:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint chp05_03/data/models/checkpoint_best.pt (epoch 2 @ 2680 updates, score 3.257) (writing took 5.20062427799985 seconds)\n",
      "2022-12-07 11:09:43 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-12-07 11:09:43 | INFO | train | epoch 002 | loss 5.874 | ppl 58.65 | wps 36222.2 | ups 3.12 | wpb 11593.8 | bsz 444.9 | num_updates 2680 | lr 0.000134 | gnorm 3.525 | loss_scale 8 | train_wall 419 | gb_free 10.1 | wall 856\n",
      "2022-12-07 11:09:43 | INFO | fairseq_cli.train | done training in 855.7 seconds\n",
      "--------------------------------------------------\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if models.is_dir():\n",
    "    print(f\"fairseq-train model exists at {models}\")\n",
    "\n",
    "else:\n",
    "\n",
    "    if IS_COLAB:\n",
    "        print(\"Start training with fairseq-train on COLAB\")\n",
    "        HR()\n",
    "        !fairseq-train \\\n",
    "            {bin} \\\n",
    "            --max-tokens 16384 \\\n",
    "            --arch transformer \\\n",
    "            --encoder-layers 4 \\\n",
    "            --decoder-layers 4 \\\n",
    "            --encoder-embed-dim 512 \\\n",
    "            --decoder-embed-dim 512 \\\n",
    "            --encoder-ffn-embed-dim 2048 \\\n",
    "            --decoder-ffn-embed-dim 2048 \\\n",
    "            --encoder-attention-heads 8 \\\n",
    "            --decoder-attention-heads 8 \\\n",
    "            --optimizer adam --lr 2e-4 \\\n",
    "            --lr-scheduler inverse_sqrt \\\n",
    "            --warmup-updates 4000 \\\n",
    "            --save-dir {models} \\\n",
    "            --max-epoch 2 \\\n",
    "            --reset-optimizer \\\n",
    "            --no-epoch-checkpoints \\\n",
    "            --fp16\n",
    "\n",
    "            # --max-epoch 10 \\\n",
    "            \n",
    "    else:\n",
    "\n",
    "        print(\"Start training with fairseq-train locally (non-GPU)\")\n",
    "        HR()\n",
    "        !fairseq-train \\\n",
    "            {bin} \\\n",
    "            --max-tokens 16384 \\\n",
    "            --arch transformer \\\n",
    "            --encoder-layers 4 \\\n",
    "            --decoder-layers 4 \\\n",
    "            --encoder-embed-dim 512 \\\n",
    "            --decoder-embed-dim 512 \\\n",
    "            --encoder-ffn-embed-dim 2048 \\\n",
    "            --decoder-ffn-embed-dim 2048 \\\n",
    "            --encoder-attention-heads 8 \\\n",
    "            --decoder-attention-heads 8 \\\n",
    "            --optimizer adam --lr 2e-4 \\\n",
    "            --lr-scheduler inverse_sqrt \\\n",
    "            --warmup-updates 4000 \\\n",
    "            --save-dir {models} \\\n",
    "            --max-epoch 1 \\\n",
    "            --no-epoch-checkpoints \\\n",
    "            --cpu\n",
    "\n",
    "    HR()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a7799dc-e7ab-4277-9a1e-5d5081cd03d5",
   "metadata": {
    "id": "1a7799dc-e7ab-4277-9a1e-5d5081cd03d5"
   },
   "outputs": [],
   "source": [
    "sp_tatoeba = spm.SentencePieceProcessor(model_file=f\"{sp_tokenizer_models}/tatoeba_cc100.rom.spm.model\")\n",
    "\n",
    "test_sentences = [\n",
    "    'Kishaninoru.',\n",
    "    'Shinbunkisha.',\n",
    "    'Nunodefuku.',\n",
    "    'Furuutowofuku.',\n",
    "    'Nunodefuruutowofuku.',\n",
    "    'Kikaigakushuuwobenkyousurumatatonaikikai.',\n",
    "    'Jinkouchinounikyouminoarujinkougafueteiru.',\n",
    "    'Kougakubunogakuhiwahijounikougakuninatta.',\n",
    "    'Reizoukonishougaganainarashouganai.',\n",
    "    'Chikatetsunarimasueki.',\n",
    "    'Karumenmenyoripaeriasuki.'\n",
    "]\n",
    "\n",
    "rom_tokenized = [' '.join(sp_tatoeba.id_to_piece(sp_tatoeba.encode(rom))) for rom in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "r1voRszPKxsU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1voRszPKxsU",
    "outputId": "c81b497b-86ed-4e46-d704-c7d45f1cd141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Ki sha ni no ru .\n",
      "▁Shi n bun ki sha .\n",
      "▁ N u node fuku .\n",
      "▁Fu ru u to wo fuku .\n",
      "▁ N u node fu ru u to wo fuku .\n",
      "▁Ki kai gaku shuu wo benkyou suru mata to nai ki kai .\n",
      "▁Ji n kou chi nou ni kyou mi no aru jin kou ga fu e teiru .\n",
      "▁Kou gaku bu no gaku hi wa hijouni kou gaku ninatta .\n",
      "▁ R ei zou ko ni shou ga ganai nara shou ganai .\n",
      "▁Chi ka te tsu n arimasu eki .\n",
      "▁Ka ru men men yori pa e ri a suki .\n"
     ]
    }
   ],
   "source": [
    "!echo \"{'\\n'.join(rom_tokenized)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "xzbJNvo2J9gO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzbJNvo2J9gO",
    "outputId": "2e82db33-69a5-4e5d-f3da-d4fa959df144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-07 10:40:50 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'chp05_03/data/models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'chp05_03/data/bin', 'source_lang': 'rom', 'target_lang': 'kan', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-12-07 10:40:51 | INFO | fairseq.tasks.translation | [rom] dictionary: 1008 types\n",
      "2022-12-07 10:40:51 | INFO | fairseq.tasks.translation | [kan] dictionary: 10000 types\n",
      "2022-12-07 10:40:51 | INFO | fairseq_cli.interactive | loading model(s) from chp05_03/data/models/checkpoint_best.pt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-interactive\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/fairseq_cli/interactive.py\", line 313, in cli_main\n",
      "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/fairseq_cli/interactive.py\", line 145, in main\n",
      "    models, _model_args = checkpoint_utils.load_model_ensemble(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/fairseq/checkpoint_utils.py\", line 367, in load_model_ensemble\n",
      "    ensemble, args, _task = load_model_ensemble_and_task(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/fairseq/checkpoint_utils.py\", line 423, in load_model_ensemble_and_task\n",
      "    raise IOError(\"Model file not found: {}\".format(filename))\n",
      "OSError: Model file not found: chp05_03/data/models/checkpoint_best.pt\n"
     ]
    }
   ],
   "source": [
    "!echo \"{'\\n'.join(rom_tokenized)}\" | fairseq-interactive \\\n",
    "{bin} \\\n",
    "--path {models}/checkpoint_best.pt \\\n",
    "--source-lang rom \\\n",
    "--target-lang kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "zOd5g0zpoFDx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOd5g0zpoFDx",
    "outputId": "705e54cf-1c7d-464d-d189-59196e711228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!echo \"{'\\n'.join(rom_tokenized)}\" | fairseq-interactive \\\n",
    "{bin} \\\n",
    "--path {models}/checkpoint_best.pt \\\n",
    "--source-lang rom \\\n",
    "--target-lang kan \\\n",
    "--beam 10 2> /dev/null | grep 'H-' | cut -f3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAzUAg28zH4L",
   "metadata": {
    "id": "wAzUAg28zH4L"
   },
   "source": [
    "<a name='5.3.4'></a><a id='5.3.4'></a>\n",
    "## 5.3.4 Checking created artifacts\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "KZQEbwMHzDEc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZQEbwMHzDEc",
    "outputId": "2018e17c-683e-470d-ae6e-6dfe9c49cfb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp05_03\n",
      "├── data\n",
      "│   ├── bin\n",
      "│   │   ├── dict.kan.txt\n",
      "│   │   ├── dict.rom.txt\n",
      "│   │   ├── preprocess.log\n",
      "│   │   ├── train.rom-kan.kan.bin\n",
      "│   │   ├── train.rom-kan.kan.idx\n",
      "│   │   ├── train.rom-kan.rom.bin\n",
      "│   │   ├── train.rom-kan.rom.idx\n",
      "│   │   ├── valid.rom-kan.kan.bin\n",
      "│   │   ├── valid.rom-kan.kan.idx\n",
      "│   │   ├── valid.rom-kan.rom.bin\n",
      "│   │   └── valid.rom-kan.rom.idx\n",
      "│   ├── cc100.ja.mod1k.txt\n",
      "│   ├── models\n",
      "│   ├── raw_text\n",
      "│   │   ├── tatoeba_cc100.kan\n",
      "│   │   └── tatoeba_cc100.rom\n",
      "│   ├── sentences20210924.tar.bz2\n",
      "│   ├── sentences.csv\n",
      "│   ├── sentences.jpn\n",
      "│   ├── sp_tokenizer_models\n",
      "│   │   ├── tatoeba_cc100.kan.spm.model\n",
      "│   │   ├── tatoeba_cc100.kan.spm.vocab\n",
      "│   │   ├── tatoeba_cc100.rom.spm.model\n",
      "│   │   └── tatoeba_cc100.rom.spm.vocab\n",
      "│   └── tokenized_corpus\n",
      "│       ├── tatoeba_cc100.tok.kan\n",
      "│       ├── tatoeba_cc100.tok.rom\n",
      "│       ├── tatoeba_cc100.tok.train.kan\n",
      "│       ├── tatoeba_cc100.tok.train.rom\n",
      "│       ├── tatoeba_cc100.tok.valid.kan\n",
      "│       └── tatoeba_cc100.tok.valid.rom\n",
      "└── requirements_5_5_3.txt\n",
      "\n",
      "6 directories, 28 files\n"
     ]
    }
   ],
   "source": [
    "!tree chp05_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7D9iQpanACR",
   "metadata": {
    "id": "k7D9iQpanACR"
   },
   "source": [
    "<a name='5.3.5'></a><a id='5.3.5'></a>\n",
    "## 5.3.5 Saving artifacts to Google Drive\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "https://drive.google.com/drive/my-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "qF8yJN89TRa7",
   "metadata": {
    "id": "qF8yJN89TRa7"
   },
   "outputs": [],
   "source": [
    "# If need to start with a clean directory on Google Drive\n",
    "# !rm -fr /content/drive/MyDrive/chp05_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "iYJDu-xyRXBp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYJDu-xyRXBp",
    "outputId": "b2eecd30-697c-4037-86a6-0be744215809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "--------------------------------------------------\n",
      "Overwriting /content/drive/MyDrive/chp05_03\n",
      "--------------------------------------------------\n",
      "total 618642\n",
      "-rw------- 1 root root 633488835 Dec  7 11:21 checkpoint_best.pt\n",
      "--------------------------------------------------\n",
      "Check contents of chp05_03:\n",
      "512\t/content/drive/MyDrive/chp05_03/requirements_5_5_3.txt\n",
      "4.0K\t/content/drive/MyDrive/chp05_03/data/.ipynb_checkpoints\n",
      "830K\t/content/drive/MyDrive/chp05_03/data/sp_tokenizer_models\n",
      "12M\t/content/drive/MyDrive/chp05_03/data/sentences.jpn\n",
      "72M\t/content/drive/MyDrive/chp05_03/data/cc100.ja.mod1k.txt\n",
      "85M\t/content/drive/MyDrive/chp05_03/data/bin\n",
      "139M\t/content/drive/MyDrive/chp05_03/data/raw_text\n",
      "147M\t/content/drive/MyDrive/chp05_03/data/sentences20210924.tar.bz2\n",
      "349M\t/content/drive/MyDrive/chp05_03/data/tokenized_corpus\n",
      "515M\t/content/drive/MyDrive/chp05_03/data/sentences.csv\n",
      "605M\t/content/drive/MyDrive/chp05_03/data/models\n",
      "1.9G\t/content/drive/MyDrive/chp05_03\n",
      "1.9G\t/content/drive/MyDrive/chp05_03/data\n"
     ]
    }
   ],
   "source": [
    "# Set PUSH_TO_GD to True if you want to push chp05_03 to Google Drive \n",
    "PUSH_TO_GD = True\n",
    "if IS_COLAB and PUSH_TO_GD:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    HR()\n",
    "\n",
    "    print(\"Overwriting /content/drive/MyDrive/chp05_03\")\n",
    "    !cp -R /content/chp05_03  /content/drive/MyDrive\n",
    "    HR()\n",
    "\n",
    "    # Only keep the checkpoint_best.pt model\n",
    "    !rm -fr /content/drive/MyDrive/chp05_03/data/models/checkpoint_last.pt\n",
    "    !du -ha /content/drive/MyDrive/chp05_03/data/models\n",
    "    HR() \n",
    "\n",
    "    print(\"Check contents of chp05_03:\")\n",
    "    !du -ah /content/drive/MyDrive/chp05_03 --max-depth=2 | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vR2cn15aDrbI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vR2cn15aDrbI",
    "outputId": "f0b085fe-3584-4e19-dae4-76b40ad8029e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ 医者 に の る 。\n",
      "▁ 新聞 新聞 者 。\n",
      "▁ の ので 服 。\n",
      "▁ フル ー ル と 服 を 服 。\n",
      "▁ の ので フル ー と 服 を 服 。\n",
      "▁ 機械 学 学習 を 勉強 する と 機械 。\n",
      "▁ 工事 地 地 の 脳 に 興味 のある 人口 が 増えている 。\n",
      "▁ 高額 の 学 費 は非常に 高額 になった 。\n",
      "▁ 冷蔵庫 に 証拠 がない なら 賞 がない 。\n",
      "▁ 地下鉄 くなります 。\n",
      "▁ カル 面 面 より パパ 好き 。\n"
     ]
    }
   ],
   "source": [
    "if PUSH_TO_GD:\n",
    "    # Test that we can do inference on stored on Google Drive\n",
    "    !echo \"{'\\n'.join(rom_tokenized)}\" | fairseq-interactive \\\n",
    "    /content/drive/MyDrive/chp05_03/data/bin \\\n",
    "    --path /content/drive/MyDrive/chp05_03/data/models/checkpoint_best.pt \\\n",
    "    --source-lang rom \\\n",
    "    --target-lang kan \\\n",
    "    --beam 10 2> /dev/null | grep 'H-' | cut -f3"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
