{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10508fb7-d632-4d14-bf47-1f6cd80d3361",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 4: Word and Sentence Embeddings\n",
    "\n",
    "## 4.2 Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8605126-b290-449c-9f3b-7116e0dd760e",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/gbih/nlp/blob/main/ja_nlp_book/chp04_4_2_sentence_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee3205-d55b-42af-aa4b-50f9ae4ca6c7",
   "metadata": {},
   "source": [
    "* [Setup](#setup)\n",
    "* [4.2 Sentence Embeddings](#4.2)\n",
    "    - [4.2.1 What are Sentence Embeddings?](#4.2.1)\n",
    "    - [4.2.2 Train Sentence Embeddings with the Doc2Vec Model](#4.2.2)\n",
    "    - [4.2.3 Search by Japanese Keywords with ElasticSearch](#4.2.3)\n",
    "    - [4.2.4 Search by Sentence Embeddings with ElasticSearch](#4.2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22c0a4-b2b7-4144-8da6-c33aaf19f6b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a name='setup'></a><a id='setup'></a>\n",
    "# Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da577dd7-70f2-48d3-bff6-e81fcb0cef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp04_02 exists.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(\"chp04_02\")\n",
    "req_file = data_root / \"requirements_4_4_2.txt\"\n",
    "\n",
    "if not data_root.is_dir():\n",
    "    data_root.mkdir()\n",
    "else:\n",
    "    print(f\"{data_root} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368547cd-ef76-4239-aa75-f939d9a03159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chp04_02/requirements_4_4_2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {req_file}\n",
    "fugashi[unidic]==1.2.1\n",
    "gensim==4.2.0\n",
    "japanize_matplotlib==1.1.3\n",
    "watermark==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b707b27b-c303-4881-a691-9e1d70f6d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "check1 = ('google.colab' in sys.modules)\n",
    "check2 = (os.environ.get('CLOUDSDK_CONFIG')=='/content/.config')\n",
    "IS_COLAB = True if (check1 or check2) else False\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Installing packages\")\n",
    "    !pip install --quiet -r {req_file}\n",
    "    !python -m unidic download\n",
    "    print(\"Packages installed.\")\n",
    "else:\n",
    "    print(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f215bbed-ab01-4006-9dcf-6e7e96e33756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "japanize_matplotlib: 1.1.3\n",
      "requests           : 2.28.1\n",
      "matplotlib         : 3.6.2\n",
      "sys                : 3.8.12 (default, Dec 13 2021, 20:17:08) \n",
      "[Clang 13.0.0 (clang-1300.0.29.3)]\n",
      "tensorflow_datasets: 4.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard Library imports\n",
    "from importlib.metadata import version\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Suppress TensorFlog log messages\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import shlex\n",
    "import shutil\n",
    "import subprocess\n",
    "#from subprocess import Popen, PIPE, STDOUT\n",
    "from sys import modules\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "from elasticsearch import Elasticsearch\n",
    "from fugashi import Tagger\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "from watermark import watermark\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Examine all imported packages\n",
    "print(watermark(iversions=True, globals_=globals(),python=True, machine=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9adc218-ca0c-44a5-89f7-f84c9ff1f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported specified packages.\n"
     ]
    }
   ],
   "source": [
    "assert version('fugashi') == '1.2.1'\n",
    "assert version('gensim') == '4.2.0'\n",
    "assert version('japanize_matplotlib') == '1.1.3'\n",
    "assert version('unidic_lite') == '1.0.8'\n",
    "\n",
    "print(\"Successfully imported specified packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "172fd031-a9ed-4181-9906-c172ddf01c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data_file:\telasticsearch-7.13.0-darwin-x86_64.tar.gz\n",
      "data_url:\thttps://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.0-darwin-x86_64.tar.gz\n",
      "data_dir:\tchp04_02\n",
      "data_src:\tchp04_02/elasticsearch-7.13.0-darwin-x86_64.tar.gz\n",
      "data_path:\tchp04_02/elasticsearch-7.13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = \"elasticsearch-7.13.0-darwin-x86_64.tar.gz\"\n",
    "data_url = f\"https://artifacts.elastic.co/downloads/elasticsearch/{data_file}\"\n",
    "data_dir = Path(\"chp04_02\")\n",
    "data_src = data_dir / data_file\n",
    "data_path = data_dir / \"elasticsearch-7.13.0\"\n",
    "\n",
    "print(f\"\"\"\n",
    "data_file:\\t{data_file}\n",
    "data_url:\\t{data_url}\n",
    "data_dir:\\t{data_dir}\n",
    "data_src:\\t{data_src}\n",
    "data_path:\\t{data_path}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81ad72d-3a3a-4fba-8450-8012257d1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_wikipedia_path:\tchp04_shared/wikipedia_data\n"
     ]
    }
   ],
   "source": [
    "data_wikipedia_path = Path(\"chp04_shared/wikipedia_data\")\n",
    "print(f\"data_wikipedia_path:\\t{data_wikipedia_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53a807c-5033-4888-9bee-23b1990d4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_dir:\tchp04_02/models\n",
      "model_path:\tchp04_02/models/Doc2Vec_model_4_2_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dir = data_dir / \"models\"\n",
    "model_path = model_dir / \"Doc2Vec_model_4_2_2\"\n",
    "print(f\"\"\"\n",
    "model_dir:\\t{model_dir}\n",
    "model_path:\\t{model_path}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ca16a-0941-4039-b26e-ecc4ec25e2db",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='4.2'></a><a id='4.2'></a>\n",
    "# 4.2 Sentence Embeddings\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1da1d5-4fda-4fec-a640-c55f46267c36",
   "metadata": {},
   "source": [
    "<a name='4.2.1'></a><a id='4.2.1'></a>\n",
    "## 4.2.1 What are Sentence Embeddings?\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "**Concept**\n",
    "\n",
    "In addition to word embeddings, we can also use sentence embeddings. These are real-valued vector representations of sentences. They are trained in such a way to capture linguistic and semantic properties of sentences.\n",
    "\n",
    "Sentence embeddings are a powerful tool that enables finding documents that do not necessarily have lexical overlap with the query. \n",
    "\n",
    "**Mechanism**\n",
    "\n",
    "There are multiple ways to compute sentence embeddings. \n",
    "\n",
    "The simplest way is to treat a sentence as a sequence of words, and average the embeddings for those words.\n",
    "\n",
    "A more sophisticated way is to use the Doc2Vec model, which treats sentences as additional variables to the Word2Vec model. This captures the semantics of sentences by predicting the words that appear inside. \n",
    "\n",
    "**Workflow**\n",
    "\n",
    "Here, we use the Doc2Vec model to train sentence embeddings from Wikipedia articles, and explore how to use the embeddings to index documents with ElasticSearch and retrieve documents semantically with vectors, not just with keywords. \n",
    "\n",
    "The main relevant Python libraries are:\n",
    "\n",
    "* elasticsearch\n",
    "* fugashi with unidic-lite or unidic\n",
    "* gensim\n",
    "* tensorflow\n",
    "* tensorflow-datasets\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The main dataset we use is the Japanese Wikipedia dump, accessed via Tensorflow Dataset. We compute one vector per document/article.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "* [Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)\n",
    "* gensim.models.doc2vec.Doc2Vec\n",
    "    - Learn paragraph and document embeddings via the distributed memory and distributed bag of words models from Quoc Le and Tomas Mikolov: “Distributed Representations of Sentences and Documents”.\n",
    "* model.dv.most_similar\n",
    "    - Find the top-N most similar keys.\n",
    "    - Positive keys contribute positively towards the similarity, negative keys negatively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4dffa-b8dd-480a-8419-f0c2f29cf53c",
   "metadata": {},
   "source": [
    "<a name='4.2.2'></a><a id='4.2.2'></a>\n",
    "## 4.2.2 Train Sentence Embeddings with the Doc2Vec Model\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d4da2-543b-4938-9abb-496737ec4463",
   "metadata": {},
   "source": [
    "### Compute one vector per document/article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3843ff-b38f-4123-8076-bdbeaf873cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use wikipedia/20200301.ja\n",
      "--------------------------------------------------\n",
      "<PrefetchDataset element_spec={'text': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "# Load the Japanese Wikipedia dump\n",
    "wikipedia_src='wikipedia/20200301.ja'\n",
    "     \n",
    "print(f\"Use {wikipedia_src}\")\n",
    "HR()\n",
    "      \n",
    "ds = tfds.load(\n",
    "    wikipedia_src, \n",
    "    split='train', \n",
    "    shuffle_files=True,\n",
    "    data_dir = data_wikipedia_path\n",
    ")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb910661-0bc6-4b88-8b65-24c5264159bd",
   "metadata": {},
   "source": [
    "### Tokenize the text into words (morphemes) with fugashi \n",
    "\n",
    "* We are creating TaggedDocument instances, which we need as input for the Doc2Vec model. \n",
    "* The sentence embeddings will be trained by creating the Doc2Vec object. \n",
    "* The resulting document embeddings are stored in `model.dv` and can be retrieved by invoking the `model.dv.most_similar()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257f0c32-32ca-4c7d-889b-78e37e1e0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:25<00:00, 392.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenizing words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = Tagger()\n",
    "\n",
    "documents = []\n",
    "for i, example in enumerate(tqdm(tfds.as_numpy(ds.take(10_000)))):\n",
    "    text = example['text'].decode('utf-8')\n",
    "    tokens = [w.surface for w in tagger(text)]\n",
    "    documents.append(TaggedDocument(tokens, [i]))\n",
    "    \n",
    "tqdm.write(\"Done tokenizing words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310e86c8-657c-4c9b-a4e6-3f156a958652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['フィリップ', '・', 'ルイス', '・', 'メイ']\n",
      "1 ['転送', 'テイラー', '・', 'セント', '・']\n",
      "2 ['喜多', '了祐', '（', 'きた', 'りょう']\n",
      "3 ['ひまわり', '温泉', '(', 'ひまわり', 'おんせん']\n",
      "4 ['高田', 'ドブロク', '事件', '（', 'たか']\n"
     ]
    }
   ],
   "source": [
    "# Examine the document titles\n",
    "for i, x in enumerate(documents[:5]):\n",
    "    print(i, documents[i][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9071f475-17e6-465c-ad6e-6fcbda2b4915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp04_shared/wikipedia_data exists.\n",
      "chp04_02/models exists.\n"
     ]
    }
   ],
   "source": [
    "# Setup for model and wikipedia directories\n",
    "if not Path(data_wikipedia_path).is_dir():\n",
    "    data_wikipedia_path.mkdir(parents=True, exist_ok=False)\n",
    "else:\n",
    "    print(f\"{data_wikipedia_path} exists.\")\n",
    "    \n",
    "if not Path(model_dir).is_dir():\n",
    "    model_dir.mkdir(parents=True, exist_ok=False)\n",
    "else:\n",
    "    print(f\"{model_dir} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5313346-1b7c-4993-aeed-28e372aae713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp04_02/models/Doc2Vec_model_4_2_2 exists\n"
     ]
    }
   ],
   "source": [
    "if not Path(model_path).is_file():\n",
    "    print(f\"Creating and training model {model_path}\")\n",
    "\n",
    "    model = Doc2Vec(\n",
    "        documents,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    print(\"Done training\")\n",
    "    model.save(str(model_path))\n",
    "    \n",
    "else:\n",
    "    print(f\"{model_path} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b638a8-6cd7-47d1-ac25-ccb35efd8f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.doc2vec.Doc2Vec"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue training with the loaded model.\n",
    "\n",
    "model = Doc2Vec.load(str(model_path))\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db55fa1-c6a1-4ac5-b35f-2e6258b81ce9",
   "metadata": {},
   "source": [
    "### Retrieve document embeddings stored in model.dv KeyedVectors\n",
    "\n",
    "* `model.dv.most_similar` is used to find the top-N most similar keys.\n",
    "* `model.dv` KeyedVectors is used to perform operations on the vectors such as vector lookup, distance, similarity etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9e29a4-84f3-429a-a622-3486885cda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(orig_doc_id):\n",
    "    \"\"\"\n",
    "    Previews 10 most similar documents for a given document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replacement for utility.truncate_by_width()\n",
    "    truncate_n = 20 \n",
    "    \n",
    "    # Show more text for the doc_id so we can infer its content type\n",
    "    w1 = ''.join(documents[orig_doc_id].words)[:truncate_n*4]\n",
    "    t1 = 'orig id = '\n",
    "    print(f\"{t1:>10}{orig_doc_id:>4}, ttl = {w1}\")\n",
    "    HR()\n",
    "    t2 = 'id = '\n",
    "    \n",
    "    # Find the top-N most similar keys per document id\n",
    "    for doc_id, sim in model.dv.most_similar(orig_doc_id): \n",
    "        w2 = ''.join(documents[doc_id].words)[:truncate_n]\n",
    "        print(f\"{t2:>10}{doc_id:>4}, ttl = {w2}, sim={sim:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b85c477-9828-43f0-bb7f-a008db41fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig id =    1, ttl = 転送テイラー・セント・クレア\n",
      "--------------------------------------------------\n",
      "     id = 3747, ttl = 飯田和孝（いいだかずたか、1982年（昭, sim=0.87\n",
      "     id = 7778, ttl = 転送シャルンホルスト級戦艦, sim=0.86\n",
      "     id =   35, ttl = 『ONE』（ワン）は、ASKAの4枚目の, sim=0.86\n",
      "     id =  528, ttl = 転送グレートディヴァイディング山脈, sim=0.86\n",
      "     id = 6120, ttl = 転送本土#日本, sim=0.85\n",
      "     id = 1474, ttl = 清水康英（しみずやすひで）は、戦国時代の, sim=0.85\n",
      "     id = 3606, ttl = 転送マックス・パーキンズ, sim=0.85\n",
      "     id = 9710, ttl = 高野切（こうやぎれ）は、『古今和歌集』の, sim=0.85\n",
      "     id =  506, ttl = アズレン(azulene)は10個の炭素, sim=0.84\n",
      "     id = 5810, ttl = 住吉台（すみよしだい）は、兵庫県神戸市東, sim=0.84\n"
     ]
    }
   ],
   "source": [
    "get_similar_docs(orig_doc_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72b14e0b-c659-4b08-9bf6-26d2c0c1e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig id =    2, ttl = 喜多了祐（きたりょうゆう、1921年-2007年）は、日本の法学者。一橋大学法学部教授・名誉教授・法学博士。北海道小樽市生まれ。研究分野は商法。中華人民共和国国\n",
      "--------------------------------------------------\n",
      "     id =  639, ttl = サンティアゴ・アリアス・ナランホ（San, sim=0.90\n",
      "     id = 2930, ttl = 南郷区（なんごうく）青森県八戸市に設置さ, sim=0.90\n",
      "     id = 5337, ttl = 川崎市立御幸中学校（かわさきしりつみゆき, sim=0.90\n",
      "     id = 7179, ttl = 転送明日はきっといい日になる#高橋優によ, sim=0.89\n",
      "     id = 4544, ttl = 聴覚障害者のみなさんへ（ちょうかくしょう, sim=0.89\n",
      "     id = 5695, ttl = 大阪東郵便局（おおさかひがしゆうびんきょ, sim=0.89\n",
      "     id = 4564, ttl = ジムロック（Simrock）は、ドイツ語, sim=0.89\n",
      "     id = 1322, ttl = みんなのうた年度別放送楽曲一覧（みんなの, sim=0.89\n",
      "     id = 7876, ttl = 小島吉蔵（吉藏、こじまきちぞう、1885, sim=0.89\n",
      "     id = 8878, ttl = 蔡元生（WinsonTsai、1969年, sim=0.88\n"
     ]
    }
   ],
   "source": [
    "get_similar_docs(orig_doc_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0ee05f4-67a6-443c-a715-688ef95b5f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig id =  123, ttl = 『トカゲの王』（トカゲのおう）は、入間人間・著、ブリキ・画のライトノベル作品。電撃文庫（アスキー・メディアワークス）より刊行されている。守月史貴・作画で電撃マオ\n",
      "--------------------------------------------------\n",
      "     id = 4743, ttl = ラリー・ユージーン・アンダーセン（Lar, sim=0.78\n",
      "     id = 1166, ttl = 転送Z会, sim=0.68\n",
      "     id = 4450, ttl = 『DDTVSサイバーエージェント路上プロ, sim=0.68\n",
      "     id = 8249, ttl = リーク郡（）は、アメリカ合衆国ミシシッピ, sim=0.66\n",
      "     id = 2145, ttl = 新生パーソナルローン株式会社（シンセイパ, sim=0.66\n",
      "     id = 3140, ttl = パトロール（Patrol）は、日産車体が, sim=0.66\n",
      "     id =  230, ttl = ヤマドリタケ(Boletusedulis, sim=0.65\n",
      "     id = 6129, ttl = 転送衝撃速報!アカルイ☆ミライ, sim=0.65\n",
      "     id = 7752, ttl = 『LostMaria-名もなき花-』（ロ, sim=0.65\n",
      "     id = 4111, ttl = 『聖闘士星矢真紅の少年伝説』（セイントセ, sim=0.65\n"
     ]
    }
   ],
   "source": [
    "get_similar_docs(orig_doc_id=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1ec48-6432-4659-8830-1184dbfc9579",
   "metadata": {},
   "source": [
    "<a name='4.2.3'></a><a id='4.2.3'></a>\n",
    "## 4.2.3 Search by Japanese Keywords with ElasticSearch\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "**Concept**\n",
    "\n",
    "Sentence embeddings is a powerful technique enabling retrieval of documents that do not necessarily have words in common with the given query.\n",
    "\n",
    "For large documents, it may be useful to integrate sentence embeddings with dedicated search engines for performance reasons. \n",
    "\n",
    "Here, we explore using sentence embeddings to index and retrieve documents using ElasticSearch, a popular search engine library.\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "* Download and run ElasticSearch as a subprocess (necessary if we run this inside a Jupyter notebook). \n",
    "* Install the plugin for Kuromoji, a popular open source softward for tokenizing Japanese text. This is often used for search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8476c-923c-46aa-8f89-c3ea1f525c21",
   "metadata": {},
   "source": [
    "### Setup Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cecc975-ba5f-496f-9c24-247a1db9c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp04_02/elasticsearch-7.13.0-darwin-x86_64.tar.gz exists.\n"
     ]
    }
   ],
   "source": [
    "if not data_src.is_file():\n",
    "    print(f\"Downloading {data_url}\")\n",
    "    subprocess.run(shlex.split(f\"wget -q -O {data_src} {data_url}\"))\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"{data_src} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c5f509-28f7-4baf-ae94-dcb8f0f88459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting chp04_02/elasticsearch-7.13.0-darwin-x86_64.tar.gz to chp04_02/elasticsearch-7.13.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if not data_path.is_dir():\n",
    "    print(f\"Extracting {data_src} to {data_path}\")\n",
    "    \n",
    "    shutil.unpack_archive(data_src, data_dir)\n",
    "    # subprocess.run(shlex.split(f\"tar -xf {data_src} -C {data_dir}\"))\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(f\"{data_path} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ee30227-3aeb-483a-846b-e132315c7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !chown ‐R daemon:daemon {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dffbe56c-3973-4a0b-813b-f95fa64bd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elasticsearch_bin:\tchp04_02/elasticsearch-7.13.0/bin\n",
      "elasticsearch_plugin:\tchp04_02/elasticsearch-7.13.0/bin/elasticsearch-plugin\n",
      "elasticsearch:\t\tchp04_02/elasticsearch-7.13.0/bin/elasticsearch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup for elasticsearch\n",
    "elasticsearch_bin = data_path / \"bin\"\n",
    "elasticsearch_plugin = f\"{elasticsearch_bin}/elasticsearch-plugin\"\n",
    "elasticsearch = f\"{elasticsearch_bin}/elasticsearch\"\n",
    "\n",
    "print(f\"\"\"\n",
    "elasticsearch_bin:\\t{elasticsearch_bin}\n",
    "elasticsearch_plugin:\\t{elasticsearch_plugin}\n",
    "elasticsearch:\\t\\t{elasticsearch}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9807ae60-8e66-4d5b-887a-078f5a61c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if any plugins are pre-installed. Should be none initially:\n",
      "--------------------------------------------------\n",
      "Install Kuromoji plugin:\n",
      "-> Installing analysis-kuromoji\n",
      "-> Downloading analysis-kuromoji from elastic\n",
      "[=================================================] 100%   \n",
      "-> Installed analysis-kuromoji\n",
      "-> Please restart Elasticsearch to activate any plugins installed\n",
      "--------------------------------------------------\n",
      "Confirm plugins:\n",
      "analysis-kuromoji\n"
     ]
    }
   ],
   "source": [
    "print(\"Check if any plugins are pre-installed. Should be none initially:\")\n",
    "!{elasticsearch_plugin} list\n",
    "HR()\n",
    "\n",
    "print(\"Install Kuromoji plugin:\")\n",
    "try:\n",
    "    subprocess.run(shlex.split(f\"{elasticsearch_plugin} install analysis-kuromoji\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error:  {e}\")\n",
    "HR()\n",
    "\n",
    "print(\"Confirm plugins:\")\n",
    "!{elasticsearch_plugin} list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d188bb-d2ac-40d1-96cb-cfcfa0c9b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  501  5109  5052   0  9:26PM ttys001    0:00.01 /bin/zsh -c ps -ef | grep elasticsearch\n",
      "  501  5111  5109   0  9:26PM ttys001    0:00.00 grep elasticsearch\n"
     ]
    }
   ],
   "source": [
    "# Make sure we don't have a previous elasticsearch daemon instance\n",
    "!ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fadfb-b69f-4b91-af5d-4fba0c0620bf",
   "metadata": {},
   "source": [
    "### Run ElasticSearch as a subprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "083cc3c1-8877-40c8-9250-9f78a0b62a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This handle errors more gracefully than es.ping\n",
    "def readiness_probe(mode=\"code\"):\n",
    "    try:\n",
    "        # Query base endpoint. This also retrieves cluster information.\n",
    "        resp = requests.get('http://localhost:9200/')\n",
    "    except Exception:\n",
    "        return False\n",
    "    else:\n",
    "        if mode == \"code\":\n",
    "            return resp\n",
    "        elif mode==\"text\":\n",
    "            return resp.text\n",
    "        \n",
    "# print(readiness_probe(mode=\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "831cf239-1703-45b5-ac9d-3bae27da1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up Elasticsearch service.................................. done in 37 seconds.\n",
      "Initialize Elasticsearch Client\n",
      "Running ping test: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gb/Desktop/python-3.8.12/env/lib/python3.8/site-packages/elasticsearch/connection/base.py:208: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "if IS_COLAB:\n",
    "    server = subprocess.Popen([elasticsearch], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=lambda: os.setuid(1))\n",
    "else:\n",
    "    server = subprocess.Popen([elasticsearch], stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
    "\n",
    "# If we use subprocess.run() inside a Jupyter notebook, it will end up hanging on the specific cell, \n",
    "# since that process is not detached. So, we use Popen since it is asynchronous, however we have to\n",
    "# wait for its start-up time. Here we use a pseudo-readiness probe\n",
    "timeout_n = 40\n",
    "print(\"Starting up Elasticsearch service.\", end = '')\n",
    "\n",
    "while not readiness_probe():\n",
    "    i += 1\n",
    "    time.sleep(1)\n",
    "    print(\".\", end='')\n",
    "    #readiness_probe()\n",
    "\n",
    "print(f\" done in {i} seconds.\")\n",
    "\n",
    "# Initialize the Python library for ElasticSearch and confirms it's running\n",
    "print(\"Initialize Elasticsearch Client\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"], \n",
    "    request_timeout=60, \n",
    "    retry_on_timeout=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"Running ping test: {es.ping()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f721096-3b56-4cc6-9dec-806dc060ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If need to kill for debugging:\n",
    "# !pkill -f 'elasticsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "927be7fc-2e94-4715-a2b5-f947487823ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"Georges-MacBook-Air.local\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"YTU83_JvTOSTJ-wmDGTD6Q\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"7.13.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"5ca8591c6fcdb1260ce95b08a8e023559635c6f3\",\n",
      "    \"build_date\" : \"2021-05-19T22:22:26.081971330Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"8.8.2\",\n",
      "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve information about the cluster.\n",
    "print(readiness_probe(mode=\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "621cc49f-9fd5-4164-ac3b-f96837b4d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete(index='wikipedia', ignore=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56396f99-354e-415e-ab13-75daa5b064a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'wikipedia'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_SETTINGS = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 3,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"kuromoji\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"kuromoji\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=\"wikipedia\", body=ES_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df2d28-f11a-4865-835b-089d411e1a17",
   "metadata": {},
   "source": [
    "### Index first 10K Japanese Wikipedia articles with Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d551dd1-c16b-4a9d-adc7-d64e13fa9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of indexing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [05:59<00:00, 27.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Start of indexing..\")\n",
    "\n",
    "for example in tqdm(tfds.as_numpy(ds.take(10_000))):\n",
    "    es.index(\n",
    "        index=\"wikipedia\",\n",
    "        body={\n",
    "            \"title\": example[\"title\"].decode(\"utf-8\"),\n",
    "            \"text\": example[\"title\"].decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "tqdm.write(\"Finished indexing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521566e3-2a07-4331-adbe-723957925348",
   "metadata": {},
   "source": [
    "### Run a query\n",
    "\n",
    "We run the query, \"自然言語\", and retrieve the most relevant documents. The matched parts will be highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c74d30c7-d4cb-4e08-8a6c-873732231a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure Elasticsearch is running\n",
    "readiness_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb339566-3d93-48d4-8774-277029941ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 1214,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 3, 'successful': 3, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 8, 'relation': 'eq'},\n",
       "  'max_score': 8.326296,\n",
       "  'hits': [{'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': '7UMoAYUBsmPYkYGKPbgW',\n",
       "    '_score': 8.326296,\n",
       "    '_source': {'title': '自然硫黄', 'text': '自然硫黄'},\n",
       "    'highlight': {'text': ['<em>自然</em>硫黄'], 'title': ['<em>自然</em>硫黄']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'm0MrAYUBsmPYkYGKZs9M',\n",
       "    '_score': 8.326296,\n",
       "    '_source': {'title': '自然成立', 'text': '自然成立'},\n",
       "    'highlight': {'text': ['<em>自然</em>成立'], 'title': ['<em>自然</em>成立']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'jEMqAYUBsmPYkYGKHsbP',\n",
       "    '_score': 6.8374434,\n",
       "    '_source': {'title': 'IETF言語タグ', 'text': 'IETF言語タグ'},\n",
       "    'highlight': {'text': ['IETF<em>言語</em>タグ'],\n",
       "     'title': ['IETF<em>言語</em>タグ']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'tUMqAYUBsmPYkYGK_swN',\n",
       "    '_score': 6.8374434,\n",
       "    '_source': {'title': 'Lucid (プログラミング言語)', 'text': 'Lucid (プログラミング言語)'},\n",
       "    'highlight': {'text': ['Lucid (プログラミング<em>言語</em>)'],\n",
       "     'title': ['Lucid (プログラミング<em>言語</em>)']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'Y0MoAYUBsmPYkYGKKbhT',\n",
       "    '_score': 6.769861,\n",
       "    '_source': {'title': 'C11 (C言語)', 'text': 'C11 (C言語)'},\n",
       "    'highlight': {'text': ['C11 (C<em>言語</em>)'],\n",
       "     'title': ['C11 (C<em>言語</em>)']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'skMqAYUBsmPYkYGKRsct',\n",
       "    '_score': 6.3118887,\n",
       "    '_source': {'title': '利根別自然公園', 'text': '利根別自然公園'},\n",
       "    'highlight': {'text': ['利根別<em>自然</em>公園'],\n",
       "     'title': ['利根別<em>自然</em>公園']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'FkMnAYUBsmPYkYGKtLUN',\n",
       "    '_score': 6.016083,\n",
       "    '_source': {'title': '禁止法 (言語学)', 'text': '禁止法 (言語学)'},\n",
       "    'highlight': {'text': ['禁止法 (<em>言語</em>学)'],\n",
       "     'title': ['禁止法 (<em>言語</em>学)']}},\n",
       "   {'_index': 'wikipedia',\n",
       "    '_type': '_doc',\n",
       "    '_id': '9EMpAYUBsmPYkYGKWMDN',\n",
       "    '_score': 5.0891905,\n",
       "    '_source': {'title': '斎藤報恩会自然史博物館', 'text': '斎藤報恩会自然史博物館'},\n",
       "    'highlight': {'text': ['斎藤報恩会<em>自然</em>史博物館'],\n",
       "     'title': ['斎藤報恩会<em>自然</em>史博物館']}}]}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_word = '自然言語'\n",
    "\n",
    "res = es.search(\n",
    "    index=\"wikipedia\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"query_string\": {\"query\": query_word}\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fragment_size\": 100,\n",
    "            \"fields\": {\n",
    "                \"title\": {},\n",
    "                \"text\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check the data structure of the returned dict\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4273803b-dd7d-4074-bffa-56593a18960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of keyword search: '自然言語'\n",
      "--------------------------------------------------\n",
      "自然硫黄\n",
      "     <em>自然</em>硫黄\n",
      "自然成立\n",
      "     <em>自然</em>成立\n",
      "IETF言語タグ\n",
      "     IETF<em>言語</em>タグ\n",
      "Lucid (プログラミング言語)\n",
      "     Lucid (プログラミング<em>言語</em>)\n",
      "C11 (C言語)\n",
      "     C11 (C<em>言語</em>)\n",
      "利根別自然公園\n",
      "     利根別<em>自然</em>公園\n",
      "禁止法 (言語学)\n",
      "     禁止法 (<em>言語</em>学)\n",
      "斎藤報恩会自然史博物館\n",
      "     斎藤報恩会<em>自然</em>史博物館\n"
     ]
    }
   ],
   "source": [
    "# Prettier output\n",
    "print(f\"Result of keyword search: '{query_word}'\")\n",
    "HR()\n",
    "for doc in res[\"hits\"][\"hits\"]:\n",
    "    if doc[\"_source\"][\"title\"]:\n",
    "        print(doc[\"_source\"][\"title\"])\n",
    "        for line in doc[\"highlight\"][\"text\"]:\n",
    "            print(\"     \"+line[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2eea5c-6299-499f-8baa-ca1b85f6c1a8",
   "metadata": {},
   "source": [
    "<a name='4.2.4'></a><a id='4.2.4'></a>\n",
    "## 4.2.4 Search by Sentence Embeddings with ElasticSearch\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Next, we index documents using sentence embedding. \n",
    "\n",
    "For this, we specify a field of type `dense_vector` when we create an index with ElasticSearch. \n",
    "\n",
    "Here, we create a field called `text_vector` of type `dense_vector` that stores 100-dimensional vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb1cd8e9-7e17-46dd-9f53-5bfa9a53045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'wikipedia-vector'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index=\"wikipedia-vector\", ignore=404)\n",
    "\n",
    "ES_SETTINGS = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 3,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"kuromoji\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"kuromoji\"\n",
    "            },\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=\"wikipedia-vector\", body=ES_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85890eeb-f421-43e1-8e39-420f44663888",
   "metadata": {},
   "source": [
    "### Add new dense_vector field to enable sentence embeddings.\n",
    "\n",
    "`doc2vec_model.dv.get_vector` returns a single unit-normalized vector for a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31bf54b4-7dcd-423e-83f7-00c38e1457b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [06:10<00:00, 27.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Wrap enumerate over tqdm\n",
    "for i, example in enumerate(tqdm(tfds.as_numpy(ds.take(10_000)))):\n",
    "    es.index(\n",
    "        index=\"wikipedia-vector\",\n",
    "        body={\n",
    "            \"title\": example[\"title\"].decode(\"utf-8\"),\n",
    "            \"text\": example[\"text\"].decode(\"utf-8\").replace(\"\\n\", \" \"),\n",
    "            \"text_vector\": model.dv.get_vector(i)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b54e71-a083-48ec-a0b5-cf3bc9d2c865",
   "metadata": {},
   "source": [
    "### Search by sentence embeddings\n",
    "\n",
    "Search by sentence embeddings by specifying a script when querying the index. \n",
    "\n",
    "Here, we use a `cosineSimilarity()` function to calculate the similarity between the query vector, and each document vector. \n",
    "\n",
    "Elasticsearch returns the same results as from the above `most_similar()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f49260e-4807-4778-96c2-94a2cde59235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_docs(orig_doc_id):\n",
    "    print(f\"orig_doc_id={orig_doc_id}, doc={' '.join(documents[orig_doc_id].words)[:40]}\")\n",
    "\n",
    "    query_vector = model.dv.get_vector(orig_doc_id)\n",
    "    \n",
    "    # Our search script\n",
    "    res = es.search(\n",
    "        index=\"wikipedia-vector\",\n",
    "        body = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        # Calculate the similarity between query vector and each document vector\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'text_vector') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "        \n",
    "    for doc in res[\"hits\"][\"hits\"]:\n",
    "        print(doc[\"_source\"][\"title\"])\n",
    "        print(\"\\t\" + (doc[\"_source\"][\"text\"])[:50])\n",
    "        HR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71e0ae4f-6bbe-40fe-b683-433afe2fd239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure Elasticsearch is running\n",
    "readiness_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e80cc216-8496-47b4-bd0f-d22ec2dfc741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_doc_id=1, doc=転送 テイラー ・ セント ・ クレア\n",
      "3Dプリンター銃製造事件\n",
      "\t3Dプリンター銃製造事件（スリーディープリンターじゅうせいぞうじけん）とは、2014年に日本で3Dプ\n",
      "--------------------------------------------------\n",
      "トミー・リピューマ\n",
      "\tトミー・リピューマ（Tommy LiPuma, 1936年7月5日 - 2017年3月13日）はアメ\n",
      "--------------------------------------------------\n",
      "1980年モスクワオリンピックのオーストラリア選手団\n",
      "\t1980年モスクワオリンピックのオーストラリア選手団は、1980年7月19日から8月3日にかけてソビ\n",
      "--------------------------------------------------\n",
      "油通し\n",
      "\t転送 揚げ物\n",
      "--------------------------------------------------\n",
      "川口ヱリサ\n",
      "\t川口 ヱリサ（かわぐち えりさ、Elisa Kawaguti、1959年6月8日 - ）は、北九州市\n",
      "--------------------------------------------------\n",
      "矢内理絵子\n",
      "\t矢内 理絵子（やうち りえこ、1980年1月10日 - ）は、日本将棋連盟所属の女流棋士。埼玉県行田\n",
      "--------------------------------------------------\n",
      "藍住町民体育館\n",
      "\t藍住町民体育館（あいずみちょうみんたいいくかん）は、徳島県板野郡藍住町にある体育館である。  藍住町\n",
      "--------------------------------------------------\n",
      "南日本造船\n",
      "\t株式会社南日本造船（みなみにっぽんぞうせん）は、大分県大分市に本社を置く今治造船関連造船会社。  概\n",
      "--------------------------------------------------\n",
      "京極高正\n",
      "\t京極 高正（きょうごく たかまさ、生没年不詳）は、江戸時代後期の高家旗本。父は京極高以。通称は鋼之丞\n",
      "--------------------------------------------------\n",
      "ロザリオとバンパイア (アニメ)\n",
      "\tロザリオとバンパイアでは、池田晃久による漫画作品『ロザリオとバンパイア』を原作としたテレビアニメ『ロ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "search_similar_docs(orig_doc_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99b1b0f7-1177-46f7-b207-7e27f0a9c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_doc_id=2, doc=喜多 了祐 （ きた りょう ゆう 、 1921 年 - 2007 年 ） は \n",
      "1576年\n",
      "\t  他の紀年法    干支 : 丙子  日本  天正4年  光永元年（私年号）  皇紀2236年  \n",
      "--------------------------------------------------\n",
      "ルカ・ラノッテ\n",
      "\tルカ・ラノッテ（, 1985年7月30日 - ）は、イタリアミラノ出身の男性(アイスダンス)選手。パ\n",
      "--------------------------------------------------\n",
      "ウィリアム・ハント\n",
      "\tウィリアム・ハント (William Hunt)   ウィリアム・ヘンリー・ハント (画家) - イ\n",
      "--------------------------------------------------\n",
      "大阪府立東大阪支援学校\n",
      "\t大阪府立東大阪支援学校（おおさかふりつ ひがしおおさかしえんがっこう）は、大阪府東大阪市中石切町三丁\n",
      "--------------------------------------------------\n",
      "民俗採集\n",
      "\t民俗採集（みんぞくさいしゅう）とは、民俗資料の収集のために現地に直接足を運び、聞き書きや参与観察をお\n",
      "--------------------------------------------------\n",
      "マイケル・ナナリー\n",
      "\tマイケル・ナナリー（Micael Nunnally、1986年7月18日 - ）は、アメリカ合衆国・\n",
      "--------------------------------------------------\n",
      "全日本学生柔道体重別団体優勝大会\n",
      "\t全日本学生柔道体重別団体優勝大会（ぜんにほんがくせいじゅうどうたいじゅうべつだんたいゆうしょういたい\n",
      "--------------------------------------------------\n",
      "くちびるモーション\n",
      "\t転送 オリエンタル・ダイヤモンド/くちびるモーション  Category:吉井和哉が制作した楽曲 C\n",
      "--------------------------------------------------\n",
      "牛島村\n",
      "\t牛島村（うしのしまそん）は、徳島県麻植郡にあった村。現在の吉野川市鴨島町の東部にあたる。  地理  \n",
      "--------------------------------------------------\n",
      "ニシン亜科\n",
      "\tニシン亜科()はニシン科の亜科の一つ。漁業上の重要種が多く、特にタイセイヨウニシン・ ・ の3種は、\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "search_similar_docs(orig_doc_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20401061-39c3-4219-a6b4-ad0bd7adf7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Elasticserver\n",
    "!pkill -f 'elasticsearch'\n",
    "readiness_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaed2c8-2b53-4dfd-97c9-5dca9244e2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
